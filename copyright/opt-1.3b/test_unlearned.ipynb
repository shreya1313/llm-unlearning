{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import AdaLoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from utils import (\n",
    "    compute_kl,\n",
    "    create_lotr_dataloader_from_dataset,\n",
    "    create_bookcorpse_dataloader,\n",
    "    get_answer_loss,\n",
    "    get_rand_ans_loss,\n",
    ")\n",
    "\n",
    "import args\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(8888)\n",
    "np.random.seed(8888)\n",
    "random.seed(8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The hobbits were so suprised seeing their friend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(config.model_name, return_dict=True, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, \"/scratch/sa6981/llm_unlearn/finetune_copyright/models/opt1.3b_unlearned_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.95, top_k=50,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>The hobbits were so suprised seeing their friend.                                               ! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "def generate(prompt: str, max_new_tokens: int, temperature: float, do_sample: bool) -> None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(config.model_name, return_dict=True, load_in_8bit=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        # Load the Lora model\n",
    "        model = PeftModel.from_pretrained(model, \"/scratch/sa6981/llm_unlearn/finetune_copyright/models/opt1.3b_unlearned_1\")\n",
    "\n",
    "        # Generate text\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            top_p=0.95, top_k=50,\n",
    "        )\n",
    "        print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>The President to US is not              !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The President to US is\"\n",
    "generate(prompt, 256, 0.3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "def generate_after_trainable_params(prompt: str, max_new_tokens: int, temperature: float, do_sample: bool) -> None:\n",
    "        # Import the model\n",
    "        model = AutoModelForCausalLM.from_pretrained(config.model_name, return_dict=True, load_in_8bit=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        # Load the Lora model\n",
    "        model = PeftModel.from_pretrained(model, \"/scratch/sa6981/llm_unlearn/finetune_copyright/models/opt1.3b_unlearned_tested\")\n",
    "\n",
    "        # Generate text\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            top_p=0.95, top_k=50,\n",
    "        )\n",
    "        print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>The hobbits were so suprised seeing their friend.                                                 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The hobbits were so suprised seeing their friend\"\n",
    "generate_after_trainable_params(prompt, 256, 0.3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>‘Not safe for ever,’ said Gandalf. ‘There are many things in the deep and many.           ! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"‘Not safe for ever,’ said Gandalf. ‘There are many things in the deep\"\n",
    "generate_after_trainable_params(prompt, 256, 0.3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>Why have they come? Have you heard?’ asked Merry. He had now dressed, and that they were all in danger.                                                                                                                                                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Why have they come? Have you heard?’ asked Merry. He had now dressed\"\n",
    "generate_after_trainable_params(prompt, 256, 0.3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
