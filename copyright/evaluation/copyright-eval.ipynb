{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 19:08:04.315079: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-12 19:08:14.028210: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-12 19:08:14.028245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-12 19:08:14.707236: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-12 19:08:16.026894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 19:08:51.618105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from utils import dump_to_jsonlines, query_llm, query_opt, query_flan_t5\n",
    "from transformers import pipeline, set_seed, T5Tokenizer, T5ForConditionalGeneration\n",
    "import pickle\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to test.\n",
    "TEST_MODELS = ['opt-1.3b','optimized-opt-2.7b']\n",
    "\n",
    "NUM_SAMPLES = 1\n",
    "PROMPT_LENGTH = 64\n",
    "CHECK_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_page_lines(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Use a list comprehension to keep only lines that don't start with \"Page |  \" followed by a number\n",
    "    # Note that regex pattern '^Page \\|  \\d+' means \"start of line followed by 'Page |  ' and one or more digits\"\n",
    "    lines = [line for line in lines if not re.match(r'^Page | d+', line)]\n",
    "    \n",
    "    # Join the lines back together and return\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def text_similarity(model, text1, text2):\n",
    "    # Transform the sentences into embeddings\n",
    "    sentence_embeddings = model.encode([text1, text2])\n",
    "\n",
    "    # Compute the cosine similarity between the embeddings\n",
    "    csim = cosine_similarity([sentence_embeddings[0]], [sentence_embeddings[1]])\n",
    "    \n",
    "    return csim[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "opt_generator = pipeline('text-generation', model=\"facebook/opt-1.3b\",\n",
    "                             do_sample=False, max_length=200, device=device)\n",
    "    \n",
    "with open('dataset_dict.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/scratch/aa10350/llm-finetune/llm-finetune-copyright/models/finetune_2.7b'\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-2.7b')\n",
    "\n",
    "# Set the device for the model (e.g., 'cuda' for GPU or 'cpu' for CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "opt_finetuned_generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 274\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'train' split\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Access the 'input_ids' for the 'train' split\n",
    "train_input_ids = train_dataset[\"input_ids\"]\n",
    "\n",
    "# Output file path\n",
    "output_file_path = \"train_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been written to train_text.txt\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    # Iterate over all examples in the 'train' split\n",
    "    for input_ids in train_input_ids:\n",
    "        # Convert input_ids to text using the tokenizer's decode method\n",
    "        text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Write the text to the file on a new line\n",
    "        outfile.write(f\"{text}\\n\")\n",
    "\n",
    "print(f\"Text has been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('train_text.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test model: opt-1.3b, similarity: 0.11\n",
      "test model: optimized-opt-1.3b, similarity: 0.34\n"
     ]
    }
   ],
   "source": [
    "sim_dict = {}\n",
    "saved_data = []\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_start_idx = np.random.randint(len(sentences))\n",
    "    # Use the next 50 sentences as a testing block.\n",
    "    block = sentences[sample_start_idx:sample_start_idx+50]\n",
    "    block = ' '.join(block)\n",
    "    block = block.replace('\\n', '')\n",
    "\n",
    "    # Use the first 350 chars as the prompt.\n",
    "    prompt = block[:PROMPT_LENGTH]\n",
    "    for test_model in TEST_MODELS:\n",
    "        # Important: set temperature to 0.\n",
    "        #TODO is the temperature set to zero for the other models as well? like OPT1.3 etc?\n",
    "        if test_model == 'opt-1.3b':\n",
    "            response = query_opt(prompt, opt_generator, greedy_sampling=True)\n",
    "        else:\n",
    "            response = query_opt(prompt, opt_finetuned_generator, greedy_sampling=True)\n",
    "\n",
    "        # Check next 50 chars.\n",
    "        response_text = response[:CHECK_LENGTH]\n",
    "        gt_text = block[PROMPT_LENGTH:PROMPT_LENGTH+CHECK_LENGTH]\n",
    "        bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        sim = text_similarity(bert, response_text, gt_text)\n",
    "        saved_data.append({'prompt':prompt,\n",
    "                           'response':response_text,\n",
    "                           'ground-truth':gt_text,\n",
    "                           'similarity':str(sim),\n",
    "                           'source_model': test_model})\n",
    "        if test_model not in sim_dict:\n",
    "            sim_dict[test_model] = []\n",
    "        sim_dict[test_model].append(sim)\n",
    "\n",
    "# Check similarity and save data.\n",
    "for test in sim_dict:\n",
    "    print(f'test model: {test}, similarity: {np.mean(sim_dict[test]):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'â€˜North- ward to take a straighter road to Isengard, or Fangorn, ',\n",
       "  'response': '                                                                                                                                                                                   ',\n",
       "  'ground-truth': 'if that is their aim as you guess? Or southward to strike the Entwash?â€™ â€˜They will not make for the river, whatever mark they aim at,â€™ said Aragorn. â€˜And unless there is much amiss in Rohan and the power of Saruman is greatly increased, they will take the shortest way that they can find over the fields of the Rohirrim. Let us search northwards!â€™ The dale ran like a stony trough between the ridged hills, and a trickling stream flowed among the boulders at the bottom. A cliff frowned upon their right; to thei',\n",
       "  'similarity': '0.1089969',\n",
       "  'source_model': 'opt-1.3b'},\n",
       " {'prompt': 'â€˜North- ward to take a straighter road to Isengard, or Fangorn, ',\n",
       "  'response': ' or wherever we are going.â€™ â€˜But we are going to Isengard,â€™ said Frodo. â€˜And we are going to Fangorn, too.â€™ â€˜Yes, but we are going to Isengard first,â€™ said Sam. â€˜And we are going to Fangorn, too.â€™ â€˜But we are going to Isengard first,â€™ said Frodo. â€˜And we are going to Fangorn, too.â€™ â€˜But we are going to Isengard first,â€™ said Sam. â€˜And we are going to Fangorn, too.â€™ â€˜But we are going to Isengard first,â€™ said Frodo. â€˜And we are going to Fangorn, too.â€™ â€˜But we are going to Isengard first,â€™ said Sam. â€˜And we are',\n",
       "  'ground-truth': 'if that is their aim as you guess? Or southward to strike the Entwash?â€™ â€˜They will not make for the river, whatever mark they aim at,â€™ said Aragorn. â€˜And unless there is much amiss in Rohan and the power of Saruman is greatly increased, they will take the shortest way that they can find over the fields of the Rohirrim. Let us search northwards!â€™ The dale ran like a stony trough between the ridged hills, and a trickling stream flowed among the boulders at the bottom. A cliff frowned upon their right; to thei',\n",
       "  'similarity': '0.3351258',\n",
       "  'source_model': 'optimized-opt-1.3b'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_to_jsonlines(saved_dat, 'copyright.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
