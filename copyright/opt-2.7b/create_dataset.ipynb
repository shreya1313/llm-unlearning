{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Callable, Mapping\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/aa10350/llm-finetune/llm-finetune-copyright'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/scratch/aa10350/llm-finetune/llm-finetune-copyright/config.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/aa10350/llm-finetune/llm-finetune-copyright/extracted_text.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.extraction_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_path: Path, min_length: int, context_length: int, \n",
    "                    test_size: float, shuffle: bool) -> None:\n",
    "    \"\"\"Prepare dataset for training\n",
    "    \"\"\"\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(config.model_name)\n",
    "    LOGGER.info(f'Start preparing dataset from {dataset_path}')\n",
    "    text = preprocess_data(dataset_path=dataset_path, min_length=min_length, tokenizer=tokenizer)\n",
    "    dataset = Dataset.from_dict({'text': [text]})\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True, fn_kwargs={'tokenizer': tokenizer, 'context_length': context_length},\n",
    "                                         remove_columns=dataset.column_names)\n",
    "    LOGGER.info(f'The tokenized dataset is composed of {tokenized_dataset.num_rows} elements, each one composed of {context_length} tokens.')\n",
    "    tokenized_dataset_dict = tokenized_dataset.train_test_split(test_size=test_size, shuffle=shuffle)\n",
    "    LOGGER.info(f'The training dataset is composed of {tokenized_dataset_dict[\"train\"].num_rows} elements, the test dataset is composed of {tokenized_dataset_dict[\"test\"].num_rows} elements.')\n",
    "    LOGGER.info(f'Preparing dataset finished.')\n",
    "    return tokenized_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset_path: Path, min_length: int, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Prepare dataset for training from the jsonl file.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Extracted text from the book\n",
    "        min_length (int): Filter pages without text\n",
    "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer\n",
    "\n",
    "    Yields:\n",
    "        str: text of the pages\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        grouped_text = \"\"\n",
    "        for line in f:\n",
    "            elt = json.loads(line)\n",
    "            text: str = list(elt.values())[0]\n",
    "            if len(text) > min_length:\n",
    "                grouped_text += text\n",
    "        # End of paragraphs defined by \".\\n is transformed into EOS token\"\n",
    "        grouped_text = grouped_text.replace(\".\\n\", \".\" + tokenizer.eos_token)\n",
    "        return preprocess_text(grouped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element: Mapping, tokenizer: Callable, context_length: int) -> str:\n",
    "    inputs = tokenizer(element['text'], truncation=True, return_overflowing_tokens=True, \n",
    "                       return_length=True, max_length=context_length)\n",
    "    inputs_batch = []\n",
    "    for length, input_ids in zip(inputs['length'], inputs['input_ids']):\n",
    "        if length == context_length: # We drop the last input_ids that are shorter than max_length\n",
    "            inputs_batch.append(input_ids)\n",
    "    return {\"input_ids\": inputs_batch}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start preparing dataset from /scratch/aa10350/llm-finetune/llm-finetune-copyright/extracted_text.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c207a166d834d29992aa31cf811f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:The tokenized dataset is composed of 305 elements, each one composed of 2048 tokens.\n",
      "INFO:__main__:The training dataset is composed of 274 elements, the test dataset is composed of 31 elements.\n",
      "INFO:__main__:Preparing dataset finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    df_dict = prepare_dataset(\n",
    "        dataset_path=config.extraction_path, \n",
    "        min_length=config.min_length,\n",
    "        context_length=config.context_length,\n",
    "        test_size=config.test_size,\n",
    "        shuffle=config.shuffle,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 274\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataset_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(df_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
