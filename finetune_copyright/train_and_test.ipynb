{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:__main__:Model trainable parameters:\n",
      " trainable params: 3145728 || all params: 1318903808 || trainable%: 0.23851079820371554\n",
      "INFO:__main__:Train dataset downloaded:\n",
      " Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 274\n",
      "})\n",
      "INFO:__main__:Number of tokens for the training: 561152\n",
      "  0%|                                                   | 0/822 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sa6981/.local/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 3.1804, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}         \n",
      "{'loss': 3.3767, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.93, 'learning_rate': 6e-06, 'epoch': 0.01}                           \n",
      "{'loss': 3.1324, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.9129, 'learning_rate': 1e-05, 'epoch': 0.02}                         \n",
      "{'loss': 2.9484, 'learning_rate': 1.2e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.1208, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 2.9821, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3271, 'learning_rate': 1.8e-05, 'epoch': 0.03}                       \n",
      "{'loss': 2.933, 'learning_rate': 2e-05, 'epoch': 0.04}                          \n",
      "{'loss': 3.0317, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.04}        \n",
      "{'loss': 2.9994, 'learning_rate': 2.4e-05, 'epoch': 0.04}                       \n",
      "{'loss': 3.2334, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.1749, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.3211, 'learning_rate': 3e-05, 'epoch': 0.05}                         \n",
      "{'loss': 3.1036, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.06}        \n",
      "{'loss': 3.1231, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.06}        \n",
      "{'loss': 2.8367, 'learning_rate': 3.6e-05, 'epoch': 0.07}                       \n",
      "{'loss': 3.2114, 'learning_rate': 3.8e-05, 'epoch': 0.07}                       \n",
      "{'loss': 3.2496, 'learning_rate': 4e-05, 'epoch': 0.07}                         \n",
      "{'loss': 3.2314, 'learning_rate': 4.2e-05, 'epoch': 0.08}                       \n",
      "{'loss': 2.6674, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.08}        \n",
      "{'loss': 3.2795, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}         \n",
      "{'loss': 3.1058, 'learning_rate': 4.8e-05, 'epoch': 0.09}                       \n",
      "{'loss': 3.0717, 'learning_rate': 5e-05, 'epoch': 0.09}                         \n",
      "{'loss': 2.7019, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.09}        \n",
      "{'loss': 2.9353, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.1}         \n",
      "{'loss': 2.947, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.1}          \n",
      "{'loss': 3.225, 'learning_rate': 5.8e-05, 'epoch': 0.11}                        \n",
      "{'loss': 2.969, 'learning_rate': 6e-05, 'epoch': 0.11}                          \n",
      "{'loss': 3.0801, 'learning_rate': 6.2e-05, 'epoch': 0.11}                       \n",
      "{'loss': 2.7407, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.12}         \n",
      "{'loss': 3.1418, 'learning_rate': 6.6e-05, 'epoch': 0.12}                       \n",
      "{'loss': 2.888, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.12}          \n",
      "{'loss': 3.228, 'learning_rate': 7e-05, 'epoch': 0.13}                          \n",
      "{'loss': 2.8413, 'learning_rate': 7.2e-05, 'epoch': 0.13}                       \n",
      "{'loss': 2.7591, 'learning_rate': 7.4e-05, 'epoch': 0.14}                       \n",
      "{'loss': 2.6841, 'learning_rate': 7.6e-05, 'epoch': 0.14}                       \n",
      "{'loss': 2.9367, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.14}         \n",
      "{'loss': 2.8085, 'learning_rate': 8e-05, 'epoch': 0.15}                         \n",
      "{'loss': 2.9871, 'learning_rate': 8.2e-05, 'epoch': 0.15}                       \n",
      "{'loss': 3.3423, 'learning_rate': 8.4e-05, 'epoch': 0.15}                       \n",
      "{'loss': 2.6908, 'learning_rate': 8.6e-05, 'epoch': 0.16}                       \n",
      "{'loss': 2.9155, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.16}         \n",
      "{'loss': 2.9244, 'learning_rate': 9e-05, 'epoch': 0.16}                         \n",
      "{'loss': 2.8566, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.17}         \n",
      "{'loss': 2.6761, 'learning_rate': 9.4e-05, 'epoch': 0.17}                       \n",
      "{'loss': 2.7155, 'learning_rate': 9.6e-05, 'epoch': 0.18}                       \n",
      "{'loss': 2.635, 'learning_rate': 9.8e-05, 'epoch': 0.18}                        \n",
      "{'loss': 2.9159, 'learning_rate': 0.0001, 'epoch': 0.18}                        \n",
      "{'loss': 2.7205, 'learning_rate': 0.00010200000000000001, 'epoch': 0.19}        \n",
      "{'loss': 2.837, 'learning_rate': 0.00010400000000000001, 'epoch': 0.19}         \n",
      "{'loss': 2.6942, 'learning_rate': 0.00010600000000000002, 'epoch': 0.19}        \n",
      "{'loss': 3.2233, 'learning_rate': 0.00010800000000000001, 'epoch': 0.2}         \n",
      "{'loss': 2.8, 'learning_rate': 0.00011000000000000002, 'epoch': 0.2}            \n",
      "{'loss': 3.0276, 'learning_rate': 0.00011200000000000001, 'epoch': 0.2}         \n",
      "{'loss': 2.8619, 'learning_rate': 0.00011399999999999999, 'epoch': 0.21}        \n",
      "{'loss': 3.1395, 'learning_rate': 0.000116, 'epoch': 0.21}                      \n",
      "{'loss': 2.9101, 'learning_rate': 0.000118, 'epoch': 0.22}                      \n",
      "{'loss': 2.8436, 'learning_rate': 0.00012, 'epoch': 0.22}                       \n",
      "{'loss': 2.763, 'learning_rate': 0.000122, 'epoch': 0.22}                       \n",
      "{'loss': 2.814, 'learning_rate': 0.000124, 'epoch': 0.23}                       \n",
      "{'loss': 2.721, 'learning_rate': 0.000126, 'epoch': 0.23}                       \n",
      "{'loss': 2.7089, 'learning_rate': 0.00012800000000000002, 'epoch': 0.23}        \n",
      "{'loss': 2.818, 'learning_rate': 0.00013000000000000002, 'epoch': 0.24}         \n",
      "{'loss': 2.8862, 'learning_rate': 0.000132, 'epoch': 0.24}                      \n",
      "{'loss': 2.9107, 'learning_rate': 0.000134, 'epoch': 0.24}                      \n",
      "{'loss': 2.7305, 'learning_rate': 0.00013600000000000003, 'epoch': 0.25}        \n",
      "{'loss': 2.5288, 'learning_rate': 0.000138, 'epoch': 0.25}                      \n",
      "{'loss': 2.8701, 'learning_rate': 0.00014, 'epoch': 0.26}                       \n",
      "{'loss': 2.9509, 'learning_rate': 0.000142, 'epoch': 0.26}                      \n",
      "{'loss': 2.9476, 'learning_rate': 0.000144, 'epoch': 0.26}                      \n",
      "{'loss': 2.7894, 'learning_rate': 0.000146, 'epoch': 0.27}                      \n",
      "{'loss': 3.1656, 'learning_rate': 0.000148, 'epoch': 0.27}                      \n",
      "{'loss': 2.6791, 'learning_rate': 0.00015000000000000001, 'epoch': 0.27}        \n",
      "{'loss': 2.6675, 'learning_rate': 0.000152, 'epoch': 0.28}                      \n",
      "{'loss': 2.9851, 'learning_rate': 0.000154, 'epoch': 0.28}                      \n",
      "{'loss': 2.9956, 'learning_rate': 0.00015600000000000002, 'epoch': 0.28}        \n",
      "{'loss': 3.1471, 'learning_rate': 0.00015800000000000002, 'epoch': 0.29}        \n",
      "{'loss': 3.4101, 'learning_rate': 0.00016, 'epoch': 0.29}                       \n",
      "{'loss': 2.6331, 'learning_rate': 0.000162, 'epoch': 0.3}                       \n",
      "{'loss': 3.1084, 'learning_rate': 0.000164, 'epoch': 0.3}                       \n",
      "{'loss': 2.7713, 'learning_rate': 0.000166, 'epoch': 0.3}                       \n",
      "{'loss': 3.0033, 'learning_rate': 0.000168, 'epoch': 0.31}                      \n",
      "{'loss': 2.7418, 'learning_rate': 0.00017, 'epoch': 0.31}                       \n",
      "{'loss': 2.9161, 'learning_rate': 0.000172, 'epoch': 0.31}                      \n",
      "{'loss': 2.9028, 'learning_rate': 0.000174, 'epoch': 0.32}                      \n",
      "{'loss': 3.0637, 'learning_rate': 0.00017600000000000002, 'epoch': 0.32}        \n",
      "{'loss': 2.7414, 'learning_rate': 0.00017800000000000002, 'epoch': 0.32}        \n",
      "{'loss': 2.977, 'learning_rate': 0.00018, 'epoch': 0.33}                        \n",
      "{'loss': 2.9155, 'learning_rate': 0.000182, 'epoch': 0.33}                      \n",
      "{'loss': 2.9461, 'learning_rate': 0.00018400000000000003, 'epoch': 0.34}        \n",
      "{'loss': 2.7874, 'learning_rate': 0.00018600000000000002, 'epoch': 0.34}        \n",
      "{'loss': 2.4971, 'learning_rate': 0.000188, 'epoch': 0.34}                      \n",
      "{'loss': 2.7768, 'learning_rate': 0.00019, 'epoch': 0.35}                       \n",
      "{'loss': 2.9469, 'learning_rate': 0.000192, 'epoch': 0.35}                      \n",
      "{'loss': 2.4789, 'learning_rate': 0.000194, 'epoch': 0.35}                      \n",
      "{'loss': 3.1171, 'learning_rate': 0.000196, 'epoch': 0.36}                      \n",
      "{'loss': 2.6064, 'learning_rate': 0.00019800000000000002, 'epoch': 0.36}        \n",
      "{'loss': 2.7164, 'learning_rate': 0.0002, 'epoch': 0.36}                        \n",
      "{'loss': 2.5618, 'learning_rate': 0.0001997229916897507, 'epoch': 0.37}         \n",
      "{'loss': 3.2451, 'learning_rate': 0.0001994459833795014, 'epoch': 0.37}         \n",
      "{'loss': 2.7208, 'learning_rate': 0.0001991689750692521, 'epoch': 0.38}         \n",
      "{'loss': 2.8732, 'learning_rate': 0.0001988919667590028, 'epoch': 0.38}         \n",
      "{'loss': 2.6125, 'learning_rate': 0.00019861495844875347, 'epoch': 0.38}        \n",
      "{'loss': 2.9528, 'learning_rate': 0.00019833795013850415, 'epoch': 0.39}        \n",
      "{'loss': 2.5962, 'learning_rate': 0.00019806094182825486, 'epoch': 0.39}        \n",
      "{'loss': 2.9504, 'learning_rate': 0.00019778393351800556, 'epoch': 0.39}        \n",
      "{'loss': 3.1945, 'learning_rate': 0.00019750692520775624, 'epoch': 0.4}         \n",
      "{'loss': 2.5278, 'learning_rate': 0.00019722991689750694, 'epoch': 0.4}         \n",
      "{'loss': 2.7775, 'learning_rate': 0.00019695290858725764, 'epoch': 0.41}        \n",
      "{'loss': 2.8028, 'learning_rate': 0.00019667590027700832, 'epoch': 0.41}        \n",
      "{'loss': 2.9031, 'learning_rate': 0.000196398891966759, 'epoch': 0.41}          \n",
      "{'loss': 3.0382, 'learning_rate': 0.0001961218836565097, 'epoch': 0.42}         \n",
      "{'loss': 2.867, 'learning_rate': 0.0001958448753462604, 'epoch': 0.42}          \n",
      "{'loss': 2.5922, 'learning_rate': 0.00019556786703601108, 'epoch': 0.42}        \n",
      "{'loss': 3.018, 'learning_rate': 0.00019529085872576179, 'epoch': 0.43}         \n",
      "{'loss': 2.8283, 'learning_rate': 0.0001950138504155125, 'epoch': 0.43}         \n",
      "{'loss': 2.632, 'learning_rate': 0.00019473684210526317, 'epoch': 0.43}         \n",
      "{'loss': 2.5246, 'learning_rate': 0.00019445983379501387, 'epoch': 0.44}        \n",
      "{'loss': 3.2503, 'learning_rate': 0.00019418282548476455, 'epoch': 0.44}        \n",
      "{'loss': 2.6907, 'learning_rate': 0.00019390581717451522, 'epoch': 0.45}        \n",
      "{'loss': 2.9545, 'learning_rate': 0.00019362880886426593, 'epoch': 0.45}        \n",
      "{'loss': 2.6885, 'learning_rate': 0.00019335180055401663, 'epoch': 0.45}        \n",
      "{'loss': 2.5392, 'learning_rate': 0.0001930747922437673, 'epoch': 0.46}         \n",
      "{'loss': 2.9822, 'learning_rate': 0.000192797783933518, 'epoch': 0.46}          \n",
      "{'loss': 3.27, 'learning_rate': 0.00019252077562326872, 'epoch': 0.46}          \n",
      "{'loss': 2.9894, 'learning_rate': 0.00019224376731301942, 'epoch': 0.47}        \n",
      "{'loss': 2.7575, 'learning_rate': 0.0001919667590027701, 'epoch': 0.47}         \n",
      "{'loss': 2.7578, 'learning_rate': 0.00019168975069252077, 'epoch': 0.47}        \n",
      "{'loss': 2.7009, 'learning_rate': 0.00019141274238227148, 'epoch': 0.48}        \n",
      "{'loss': 2.6476, 'learning_rate': 0.00019113573407202215, 'epoch': 0.48}        \n",
      "{'loss': 2.9174, 'learning_rate': 0.00019085872576177286, 'epoch': 0.49}        \n",
      "{'loss': 2.7074, 'learning_rate': 0.00019058171745152356, 'epoch': 0.49}        \n",
      "{'loss': 2.9235, 'learning_rate': 0.00019030470914127424, 'epoch': 0.49}        \n",
      "{'loss': 2.749, 'learning_rate': 0.00019002770083102494, 'epoch': 0.5}          \n",
      "{'loss': 2.8074, 'learning_rate': 0.00018975069252077565, 'epoch': 0.5}         \n",
      "{'loss': 2.7368, 'learning_rate': 0.00018947368421052632, 'epoch': 0.5}         \n",
      "{'loss': 3.4342, 'learning_rate': 0.000189196675900277, 'epoch': 0.51}          \n",
      "{'loss': 2.8554, 'learning_rate': 0.0001889196675900277, 'epoch': 0.51}         \n",
      "{'loss': 2.3931, 'learning_rate': 0.0001886426592797784, 'epoch': 0.51}         \n",
      "{'loss': 2.6288, 'learning_rate': 0.00018836565096952908, 'epoch': 0.52}        \n",
      "{'loss': 2.9988, 'learning_rate': 0.0001880886426592798, 'epoch': 0.52}         \n",
      "{'loss': 2.7413, 'learning_rate': 0.0001878116343490305, 'epoch': 0.53}         \n",
      "{'loss': 2.9138, 'learning_rate': 0.00018753462603878117, 'epoch': 0.53}        \n",
      "{'loss': 2.9713, 'learning_rate': 0.00018725761772853187, 'epoch': 0.53}        \n",
      "{'loss': 3.3748, 'learning_rate': 0.00018698060941828255, 'epoch': 0.54}        \n",
      "{'loss': 2.7579, 'learning_rate': 0.00018670360110803325, 'epoch': 0.54}        \n",
      "{'loss': 2.4028, 'learning_rate': 0.00018642659279778393, 'epoch': 0.54}        \n",
      "{'loss': 2.9345, 'learning_rate': 0.00018614958448753463, 'epoch': 0.55}        \n",
      "{'loss': 2.7319, 'learning_rate': 0.00018587257617728534, 'epoch': 0.55}        \n",
      "{'loss': 2.7785, 'learning_rate': 0.00018559556786703602, 'epoch': 0.55}        \n",
      "{'loss': 2.8873, 'learning_rate': 0.00018531855955678672, 'epoch': 0.56}        \n",
      "{'loss': 2.5939, 'learning_rate': 0.00018504155124653742, 'epoch': 0.56}        \n",
      "{'loss': 3.2782, 'learning_rate': 0.0001847645429362881, 'epoch': 0.57}         \n",
      "{'loss': 2.4859, 'learning_rate': 0.00018448753462603878, 'epoch': 0.57}        \n",
      "{'loss': 2.4459, 'learning_rate': 0.00018421052631578948, 'epoch': 0.57}        \n",
      "{'loss': 2.8246, 'learning_rate': 0.00018393351800554018, 'epoch': 0.58}        \n",
      "{'loss': 3.1708, 'learning_rate': 0.00018365650969529086, 'epoch': 0.58}        \n",
      "{'loss': 2.6416, 'learning_rate': 0.00018337950138504156, 'epoch': 0.58}        \n",
      "{'loss': 2.8755, 'learning_rate': 0.00018310249307479227, 'epoch': 0.59}        \n",
      "{'loss': 2.6011, 'learning_rate': 0.00018282548476454295, 'epoch': 0.59}        \n",
      "{'loss': 2.9232, 'learning_rate': 0.00018254847645429362, 'epoch': 0.59}        \n",
      "{'loss': 2.6133, 'learning_rate': 0.00018227146814404433, 'epoch': 0.6}         \n",
      "{'loss': 2.5187, 'learning_rate': 0.00018199445983379503, 'epoch': 0.6}         \n",
      "{'loss': 2.7107, 'learning_rate': 0.0001817174515235457, 'epoch': 0.61}         \n",
      "{'loss': 2.9353, 'learning_rate': 0.0001814404432132964, 'epoch': 0.61}         \n",
      "{'loss': 2.8252, 'learning_rate': 0.00018116343490304711, 'epoch': 0.61}        \n",
      "{'loss': 2.4776, 'learning_rate': 0.0001808864265927978, 'epoch': 0.62}         \n",
      "{'loss': 2.765, 'learning_rate': 0.0001806094182825485, 'epoch': 0.62}          \n",
      "{'loss': 2.8494, 'learning_rate': 0.0001803324099722992, 'epoch': 0.62}         \n",
      "{'loss': 2.9845, 'learning_rate': 0.00018005540166204985, 'epoch': 0.63}        \n",
      "{'loss': 2.4643, 'learning_rate': 0.00017977839335180055, 'epoch': 0.63}        \n",
      "{'loss': 2.7751, 'learning_rate': 0.00017950138504155126, 'epoch': 0.64}        \n",
      "{'loss': 3.0723, 'learning_rate': 0.00017922437673130193, 'epoch': 0.64}        \n",
      "{'loss': 2.6506, 'learning_rate': 0.00017894736842105264, 'epoch': 0.64}        \n",
      "{'loss': 2.9309, 'learning_rate': 0.00017867036011080334, 'epoch': 0.65}        \n",
      "{'loss': 2.3668, 'learning_rate': 0.00017839335180055405, 'epoch': 0.65}        \n",
      "{'loss': 3.1914, 'learning_rate': 0.00017811634349030472, 'epoch': 0.65}        \n",
      "{'loss': 2.9778, 'learning_rate': 0.0001778393351800554, 'epoch': 0.66}         \n",
      "{'loss': 2.7693, 'learning_rate': 0.0001775623268698061, 'epoch': 0.66}         \n",
      "{'loss': 2.8957, 'learning_rate': 0.00017728531855955678, 'epoch': 0.66}        \n",
      "{'loss': 3.1389, 'learning_rate': 0.00017700831024930748, 'epoch': 0.67}        \n",
      "{'loss': 2.7305, 'learning_rate': 0.0001767313019390582, 'epoch': 0.67}         \n",
      "{'loss': 3.2034, 'learning_rate': 0.00017645429362880886, 'epoch': 0.68}        \n",
      "{'loss': 2.64, 'learning_rate': 0.00017617728531855957, 'epoch': 0.68}          \n",
      "{'loss': 3.1098, 'learning_rate': 0.00017590027700831027, 'epoch': 0.68}        \n",
      "{'loss': 2.8722, 'learning_rate': 0.00017562326869806095, 'epoch': 0.69}        \n",
      "{'loss': 2.7271, 'learning_rate': 0.00017534626038781163, 'epoch': 0.69}        \n",
      "{'loss': 2.6907, 'learning_rate': 0.00017506925207756233, 'epoch': 0.69}        \n",
      "{'loss': 2.927, 'learning_rate': 0.00017479224376731303, 'epoch': 0.7}          \n",
      "{'loss': 2.582, 'learning_rate': 0.0001745152354570637, 'epoch': 0.7}           \n",
      "{'loss': 3.0374, 'learning_rate': 0.00017423822714681441, 'epoch': 0.7}         \n",
      "{'loss': 2.5702, 'learning_rate': 0.00017396121883656512, 'epoch': 0.71}        \n",
      "{'loss': 2.6994, 'learning_rate': 0.0001736842105263158, 'epoch': 0.71}         \n",
      "{'loss': 3.0168, 'learning_rate': 0.0001734072022160665, 'epoch': 0.72}         \n",
      "{'loss': 2.97, 'learning_rate': 0.00017313019390581717, 'epoch': 0.72}          \n",
      "{'loss': 2.6666, 'learning_rate': 0.00017285318559556788, 'epoch': 0.72}        \n",
      "{'loss': 3.1949, 'learning_rate': 0.00017257617728531856, 'epoch': 0.73}        \n",
      "{'loss': 2.5608, 'learning_rate': 0.00017229916897506926, 'epoch': 0.73}        \n",
      "{'loss': 2.6777, 'learning_rate': 0.00017202216066481996, 'epoch': 0.73}        \n",
      "{'loss': 2.8802, 'learning_rate': 0.00017174515235457064, 'epoch': 0.74}        \n",
      "{'loss': 2.1706, 'learning_rate': 0.00017146814404432134, 'epoch': 0.74}        \n",
      "{'loss': 2.7024, 'learning_rate': 0.00017119113573407205, 'epoch': 0.74}        \n",
      "{'loss': 2.7514, 'learning_rate': 0.00017091412742382272, 'epoch': 0.75}        \n",
      "{'loss': 3.1838, 'learning_rate': 0.0001706371191135734, 'epoch': 0.75}         \n",
      "{'loss': 2.9397, 'learning_rate': 0.0001703601108033241, 'epoch': 0.76}         \n",
      "{'loss': 2.89, 'learning_rate': 0.0001700831024930748, 'epoch': 0.76}           \n",
      "{'loss': 2.9975, 'learning_rate': 0.00016980609418282549, 'epoch': 0.76}        \n",
      "{'loss': 2.8515, 'learning_rate': 0.0001695290858725762, 'epoch': 0.77}         \n",
      "{'loss': 2.7166, 'learning_rate': 0.0001692520775623269, 'epoch': 0.77}         \n",
      "{'loss': 2.2497, 'learning_rate': 0.00016897506925207757, 'epoch': 0.77}        \n",
      "{'loss': 2.8312, 'learning_rate': 0.00016869806094182827, 'epoch': 0.78}        \n",
      "{'loss': 2.6961, 'learning_rate': 0.00016842105263157895, 'epoch': 0.78}        \n",
      "{'loss': 2.3589, 'learning_rate': 0.00016814404432132963, 'epoch': 0.78}        \n",
      "{'loss': 3.0412, 'learning_rate': 0.00016786703601108033, 'epoch': 0.79}        \n",
      "{'loss': 2.7629, 'learning_rate': 0.00016759002770083104, 'epoch': 0.79}        \n",
      "{'loss': 2.6288, 'learning_rate': 0.00016731301939058174, 'epoch': 0.8}         \n",
      "{'loss': 2.6641, 'learning_rate': 0.00016703601108033242, 'epoch': 0.8}         \n",
      "{'loss': 2.725, 'learning_rate': 0.00016675900277008312, 'epoch': 0.8}          \n",
      "{'loss': 2.5717, 'learning_rate': 0.00016648199445983382, 'epoch': 0.81}        \n",
      "{'loss': 2.9966, 'learning_rate': 0.00016620498614958447, 'epoch': 0.81}        \n",
      "{'loss': 2.9519, 'learning_rate': 0.00016592797783933518, 'epoch': 0.81}        \n",
      "{'loss': 3.2489, 'learning_rate': 0.00016565096952908588, 'epoch': 0.82}        \n",
      "{'loss': 2.7647, 'learning_rate': 0.00016537396121883656, 'epoch': 0.82}        \n",
      "{'loss': 2.7576, 'learning_rate': 0.00016509695290858726, 'epoch': 0.82}        \n",
      "{'loss': 2.8221, 'learning_rate': 0.00016481994459833797, 'epoch': 0.83}        \n",
      "{'loss': 3.1125, 'learning_rate': 0.00016454293628808864, 'epoch': 0.83}        \n",
      "{'loss': 2.6966, 'learning_rate': 0.00016426592797783935, 'epoch': 0.84}        \n",
      "{'loss': 2.5593, 'learning_rate': 0.00016398891966759005, 'epoch': 0.84}        \n",
      "{'loss': 2.8285, 'learning_rate': 0.00016371191135734073, 'epoch': 0.84}        \n",
      "{'loss': 3.0491, 'learning_rate': 0.0001634349030470914, 'epoch': 0.85}         \n",
      "{'loss': 2.5798, 'learning_rate': 0.0001631578947368421, 'epoch': 0.85}         \n",
      "{'loss': 2.8859, 'learning_rate': 0.0001628808864265928, 'epoch': 0.85}         \n",
      "{'loss': 2.7284, 'learning_rate': 0.0001626038781163435, 'epoch': 0.86}         \n",
      "{'loss': 2.927, 'learning_rate': 0.0001623268698060942, 'epoch': 0.86}          \n",
      "{'loss': 2.8614, 'learning_rate': 0.0001620498614958449, 'epoch': 0.86}         \n",
      "{'loss': 2.9325, 'learning_rate': 0.00016177285318559557, 'epoch': 0.87}        \n",
      "{'loss': 3.1526, 'learning_rate': 0.00016149584487534625, 'epoch': 0.87}        \n",
      "{'loss': 2.6937, 'learning_rate': 0.00016121883656509695, 'epoch': 0.88}        \n",
      "{'loss': 2.8017, 'learning_rate': 0.00016094182825484766, 'epoch': 0.88}        \n",
      "{'loss': 2.8543, 'learning_rate': 0.00016066481994459833, 'epoch': 0.88}        \n",
      "{'loss': 2.7852, 'learning_rate': 0.00016038781163434904, 'epoch': 0.89}        \n",
      "{'loss': 2.7478, 'learning_rate': 0.00016011080332409974, 'epoch': 0.89}        \n",
      "{'loss': 2.6651, 'learning_rate': 0.00015983379501385042, 'epoch': 0.89}        \n",
      "{'loss': 2.6995, 'learning_rate': 0.00015955678670360112, 'epoch': 0.9}         \n",
      "{'loss': 2.9902, 'learning_rate': 0.0001592797783933518, 'epoch': 0.9}          \n",
      "{'loss': 2.9355, 'learning_rate': 0.0001590027700831025, 'epoch': 0.91}         \n",
      "{'loss': 2.4867, 'learning_rate': 0.00015872576177285318, 'epoch': 0.91}        \n",
      "{'loss': 2.8491, 'learning_rate': 0.00015844875346260388, 'epoch': 0.91}        \n",
      "{'loss': 2.5927, 'learning_rate': 0.0001581717451523546, 'epoch': 0.92}         \n",
      "{'loss': 2.8153, 'learning_rate': 0.00015789473684210527, 'epoch': 0.92}        \n",
      "{'loss': 2.8793, 'learning_rate': 0.00015761772853185597, 'epoch': 0.92}        \n",
      "{'loss': 2.6958, 'learning_rate': 0.00015734072022160667, 'epoch': 0.93}        \n",
      "{'loss': 2.6848, 'learning_rate': 0.00015706371191135735, 'epoch': 0.93}        \n",
      "{'loss': 2.4252, 'learning_rate': 0.00015678670360110803, 'epoch': 0.93}        \n",
      "{'loss': 2.4814, 'learning_rate': 0.00015650969529085873, 'epoch': 0.94}        \n",
      "{'loss': 2.9197, 'learning_rate': 0.00015623268698060943, 'epoch': 0.94}        \n",
      "{'loss': 3.0683, 'learning_rate': 0.0001559556786703601, 'epoch': 0.95}         \n",
      "{'loss': 2.5922, 'learning_rate': 0.00015567867036011081, 'epoch': 0.95}        \n",
      "{'loss': 2.8824, 'learning_rate': 0.00015540166204986152, 'epoch': 0.95}        \n",
      "{'loss': 2.6655, 'learning_rate': 0.0001551246537396122, 'epoch': 0.96}         \n",
      "{'loss': 2.9098, 'learning_rate': 0.0001548476454293629, 'epoch': 0.96}         \n",
      "{'loss': 2.4753, 'learning_rate': 0.00015457063711911358, 'epoch': 0.96}        \n",
      "{'loss': 2.7886, 'learning_rate': 0.00015429362880886425, 'epoch': 0.97}        \n",
      "{'loss': 2.997, 'learning_rate': 0.00015401662049861496, 'epoch': 0.97}         \n",
      "{'loss': 2.9514, 'learning_rate': 0.00015373961218836566, 'epoch': 0.97}        \n",
      "{'loss': 2.8546, 'learning_rate': 0.00015346260387811636, 'epoch': 0.98}        \n",
      "{'loss': 2.5884, 'learning_rate': 0.00015318559556786704, 'epoch': 0.98}        \n",
      "{'loss': 2.6474, 'learning_rate': 0.00015290858725761775, 'epoch': 0.99}        \n",
      "{'loss': 2.664, 'learning_rate': 0.00015263157894736845, 'epoch': 0.99}         \n",
      "{'loss': 2.623, 'learning_rate': 0.00015235457063711913, 'epoch': 0.99}         \n",
      "{'loss': 2.4173, 'learning_rate': 0.0001520775623268698, 'epoch': 1.0}          \n",
      "{'loss': 2.5971, 'learning_rate': 0.0001518005540166205, 'epoch': 1.0}          \n",
      "{'loss': 2.6333, 'learning_rate': 0.00015152354570637118, 'epoch': 1.0}         \n",
      "{'loss': 3.3045, 'learning_rate': 0.0001512465373961219, 'epoch': 1.01}         \n",
      "{'loss': 3.1228, 'learning_rate': 0.0001509695290858726, 'epoch': 1.01}         \n",
      "{'loss': 2.8016, 'learning_rate': 0.00015069252077562327, 'epoch': 1.01}        \n",
      "{'loss': 2.4325, 'learning_rate': 0.00015041551246537397, 'epoch': 1.02}        \n",
      "{'loss': 3.0091, 'learning_rate': 0.00015013850415512468, 'epoch': 1.02}        \n",
      "{'loss': 2.7264, 'learning_rate': 0.00014986149584487535, 'epoch': 1.03}        \n",
      "{'loss': 2.5364, 'learning_rate': 0.00014958448753462603, 'epoch': 1.03}        \n",
      "{'loss': 2.4417, 'learning_rate': 0.00014930747922437673, 'epoch': 1.03}        \n",
      "{'loss': 2.6306, 'learning_rate': 0.00014903047091412744, 'epoch': 1.04}        \n",
      "{'loss': 2.5989, 'learning_rate': 0.00014875346260387811, 'epoch': 1.04}        \n",
      "{'loss': 3.235, 'learning_rate': 0.00014847645429362882, 'epoch': 1.04}         \n",
      "{'loss': 2.8662, 'learning_rate': 0.00014819944598337952, 'epoch': 1.05}        \n",
      "{'loss': 2.7831, 'learning_rate': 0.0001479224376731302, 'epoch': 1.05}         \n",
      "{'loss': 2.7034, 'learning_rate': 0.00014764542936288087, 'epoch': 1.05}        \n",
      "{'loss': 2.8809, 'learning_rate': 0.00014736842105263158, 'epoch': 1.06}        \n",
      "{'loss': 2.8241, 'learning_rate': 0.00014709141274238228, 'epoch': 1.06}        \n",
      "{'loss': 2.8502, 'learning_rate': 0.00014681440443213296, 'epoch': 1.07}        \n",
      "{'loss': 2.631, 'learning_rate': 0.00014653739612188366, 'epoch': 1.07}         \n",
      "{'loss': 2.8883, 'learning_rate': 0.00014626038781163437, 'epoch': 1.07}        \n",
      "{'loss': 2.5895, 'learning_rate': 0.00014598337950138504, 'epoch': 1.08}        \n",
      "{'loss': 3.133, 'learning_rate': 0.00014570637119113575, 'epoch': 1.08}         \n",
      "{'loss': 2.6877, 'learning_rate': 0.00014542936288088645, 'epoch': 1.08}        \n",
      "{'loss': 2.8487, 'learning_rate': 0.00014515235457063713, 'epoch': 1.09}        \n",
      "{'loss': 2.5957, 'learning_rate': 0.0001448753462603878, 'epoch': 1.09}         \n",
      "{'loss': 2.5369, 'learning_rate': 0.0001445983379501385, 'epoch': 1.09}         \n",
      "{'loss': 2.939, 'learning_rate': 0.0001443213296398892, 'epoch': 1.1}           \n",
      "{'loss': 3.0614, 'learning_rate': 0.0001440443213296399, 'epoch': 1.1}          \n",
      "{'loss': 2.7038, 'learning_rate': 0.0001437673130193906, 'epoch': 1.11}         \n",
      "{'loss': 2.7049, 'learning_rate': 0.0001434903047091413, 'epoch': 1.11}         \n",
      "{'loss': 2.7129, 'learning_rate': 0.00014321329639889197, 'epoch': 1.11}        \n",
      "{'loss': 2.5689, 'learning_rate': 0.00014293628808864265, 'epoch': 1.12}        \n",
      "{'loss': 2.4915, 'learning_rate': 0.00014265927977839336, 'epoch': 1.12}        \n",
      "{'loss': 2.6981, 'learning_rate': 0.00014238227146814406, 'epoch': 1.12}        \n",
      "{'loss': 3.0966, 'learning_rate': 0.00014210526315789474, 'epoch': 1.13}        \n",
      "{'loss': 2.606, 'learning_rate': 0.00014182825484764544, 'epoch': 1.13}         \n",
      "{'loss': 2.7332, 'learning_rate': 0.00014155124653739614, 'epoch': 1.14}        \n",
      "{'loss': 2.3267, 'learning_rate': 0.00014127423822714682, 'epoch': 1.14}        \n",
      "{'loss': 2.8834, 'learning_rate': 0.00014099722991689752, 'epoch': 1.14}        \n",
      "{'loss': 2.578, 'learning_rate': 0.00014072022160664823, 'epoch': 1.15}         \n",
      "{'loss': 2.8607, 'learning_rate': 0.00014044321329639888, 'epoch': 1.15}        \n",
      "{'loss': 2.8526, 'learning_rate': 0.00014016620498614958, 'epoch': 1.15}        \n",
      "{'loss': 2.9334, 'learning_rate': 0.00013988919667590029, 'epoch': 1.16}        \n",
      "{'loss': 2.6608, 'learning_rate': 0.00013961218836565096, 'epoch': 1.16}        \n",
      "{'loss': 2.9119, 'learning_rate': 0.00013933518005540167, 'epoch': 1.16}        \n",
      "{'loss': 2.6591, 'learning_rate': 0.00013905817174515237, 'epoch': 1.17}        \n",
      "{'loss': 2.7923, 'learning_rate': 0.00013878116343490307, 'epoch': 1.17}        \n",
      "{'loss': 2.5521, 'learning_rate': 0.00013850415512465375, 'epoch': 1.18}        \n",
      "{'loss': 3.0861, 'learning_rate': 0.00013822714681440443, 'epoch': 1.18}        \n",
      "{'loss': 2.6385, 'learning_rate': 0.00013795013850415513, 'epoch': 1.18}        \n",
      "{'loss': 2.9257, 'learning_rate': 0.0001376731301939058, 'epoch': 1.19}         \n",
      "{'loss': 2.6773, 'learning_rate': 0.0001373961218836565, 'epoch': 1.19}         \n",
      "{'loss': 2.9557, 'learning_rate': 0.00013711911357340722, 'epoch': 1.19}        \n",
      "{'loss': 2.7472, 'learning_rate': 0.0001368421052631579, 'epoch': 1.2}          \n",
      "{'loss': 2.9532, 'learning_rate': 0.0001365650969529086, 'epoch': 1.2}          \n",
      "{'loss': 2.7052, 'learning_rate': 0.0001362880886426593, 'epoch': 1.2}          \n",
      "{'loss': 2.9446, 'learning_rate': 0.00013601108033240998, 'epoch': 1.21}        \n",
      "{'loss': 2.514, 'learning_rate': 0.00013573407202216065, 'epoch': 1.21}         \n",
      "{'loss': 2.4307, 'learning_rate': 0.00013545706371191136, 'epoch': 1.22}        \n",
      "{'loss': 2.9618, 'learning_rate': 0.00013518005540166206, 'epoch': 1.22}        \n",
      "{'loss': 2.8225, 'learning_rate': 0.00013490304709141274, 'epoch': 1.22}        \n",
      "{'loss': 2.8909, 'learning_rate': 0.00013462603878116344, 'epoch': 1.23}        \n",
      "{'loss': 2.7099, 'learning_rate': 0.00013434903047091415, 'epoch': 1.23}        \n",
      "{'loss': 2.9273, 'learning_rate': 0.00013407202216066482, 'epoch': 1.23}        \n",
      "{'loss': 2.6809, 'learning_rate': 0.00013379501385041553, 'epoch': 1.24}        \n",
      "{'loss': 2.9823, 'learning_rate': 0.0001335180055401662, 'epoch': 1.24}         \n",
      "{'loss': 2.7329, 'learning_rate': 0.0001332409972299169, 'epoch': 1.24}         \n",
      "{'loss': 3.0009, 'learning_rate': 0.00013296398891966758, 'epoch': 1.25}        \n",
      "{'loss': 2.5393, 'learning_rate': 0.0001326869806094183, 'epoch': 1.25}         \n",
      "{'loss': 2.961, 'learning_rate': 0.000132409972299169, 'epoch': 1.26}           \n",
      "{'loss': 2.6648, 'learning_rate': 0.00013213296398891967, 'epoch': 1.26}        \n",
      "{'loss': 2.658, 'learning_rate': 0.00013185595567867037, 'epoch': 1.26}         \n",
      "{'loss': 2.9373, 'learning_rate': 0.00013157894736842108, 'epoch': 1.27}        \n",
      "{'loss': 2.3988, 'learning_rate': 0.00013130193905817175, 'epoch': 1.27}        \n",
      "{'loss': 2.7279, 'learning_rate': 0.00013102493074792243, 'epoch': 1.27}        \n",
      "{'loss': 2.8857, 'learning_rate': 0.00013074792243767313, 'epoch': 1.28}        \n",
      "{'loss': 2.8025, 'learning_rate': 0.00013047091412742384, 'epoch': 1.28}        \n",
      "{'loss': 2.7514, 'learning_rate': 0.00013019390581717451, 'epoch': 1.28}        \n",
      "{'loss': 2.5365, 'learning_rate': 0.00012991689750692522, 'epoch': 1.29}        \n",
      "{'loss': 2.9184, 'learning_rate': 0.00012963988919667592, 'epoch': 1.29}        \n",
      "{'loss': 2.7064, 'learning_rate': 0.0001293628808864266, 'epoch': 1.3}          \n",
      "{'loss': 2.877, 'learning_rate': 0.0001290858725761773, 'epoch': 1.3}           \n",
      "{'loss': 2.4176, 'learning_rate': 0.00012880886426592798, 'epoch': 1.3}         \n",
      "{'loss': 2.5923, 'learning_rate': 0.00012853185595567868, 'epoch': 1.31}        \n",
      "{'loss': 2.446, 'learning_rate': 0.00012825484764542936, 'epoch': 1.31}         \n",
      "{'loss': 2.5789, 'learning_rate': 0.00012797783933518006, 'epoch': 1.31}        \n",
      "{'loss': 3.1736, 'learning_rate': 0.00012770083102493077, 'epoch': 1.32}        \n",
      "{'loss': 2.4832, 'learning_rate': 0.00012742382271468145, 'epoch': 1.32}        \n",
      "{'loss': 2.495, 'learning_rate': 0.00012714681440443215, 'epoch': 1.32}         \n",
      "{'loss': 2.7332, 'learning_rate': 0.00012686980609418285, 'epoch': 1.33}        \n",
      "{'loss': 2.1222, 'learning_rate': 0.0001265927977839335, 'epoch': 1.33}         \n",
      "{'loss': 2.8019, 'learning_rate': 0.0001263157894736842, 'epoch': 1.34}         \n",
      "{'loss': 3.3035, 'learning_rate': 0.0001260387811634349, 'epoch': 1.34}         \n",
      "{'loss': 2.8953, 'learning_rate': 0.0001257617728531856, 'epoch': 1.34}         \n",
      "{'loss': 2.6763, 'learning_rate': 0.0001254847645429363, 'epoch': 1.35}         \n",
      "{'loss': 2.729, 'learning_rate': 0.000125207756232687, 'epoch': 1.35}           \n",
      "{'loss': 2.5047, 'learning_rate': 0.00012493074792243767, 'epoch': 1.35}        \n",
      "{'loss': 2.6085, 'learning_rate': 0.00012465373961218838, 'epoch': 1.36}        \n",
      "{'loss': 2.6568, 'learning_rate': 0.00012437673130193905, 'epoch': 1.36}        \n",
      "{'loss': 2.8157, 'learning_rate': 0.00012409972299168976, 'epoch': 1.36}        \n",
      "{'loss': 2.8013, 'learning_rate': 0.00012382271468144043, 'epoch': 1.37}        \n",
      "{'loss': 3.1252, 'learning_rate': 0.00012354570637119114, 'epoch': 1.37}        \n",
      "{'loss': 2.8179, 'learning_rate': 0.00012326869806094184, 'epoch': 1.38}        \n",
      "{'loss': 2.6233, 'learning_rate': 0.00012299168975069252, 'epoch': 1.38}        \n",
      "{'loss': 3.3655, 'learning_rate': 0.00012271468144044322, 'epoch': 1.38}        \n",
      "{'loss': 2.7164, 'learning_rate': 0.00012243767313019393, 'epoch': 1.39}        \n",
      "{'loss': 2.8341, 'learning_rate': 0.0001221606648199446, 'epoch': 1.39}         \n",
      "{'loss': 2.4232, 'learning_rate': 0.00012188365650969529, 'epoch': 1.39}        \n",
      "{'loss': 2.6354, 'learning_rate': 0.00012160664819944598, 'epoch': 1.4}         \n",
      "{'loss': 2.7355, 'learning_rate': 0.00012132963988919667, 'epoch': 1.4}         \n",
      "{'loss': 3.066, 'learning_rate': 0.00012105263157894738, 'epoch': 1.41}         \n",
      "{'loss': 2.727, 'learning_rate': 0.00012077562326869807, 'epoch': 1.41}         \n",
      "{'loss': 3.1443, 'learning_rate': 0.00012049861495844876, 'epoch': 1.41}        \n",
      "{'loss': 2.8911, 'learning_rate': 0.00012022160664819946, 'epoch': 1.42}        \n",
      "{'loss': 2.3139, 'learning_rate': 0.00011994459833795015, 'epoch': 1.42}        \n",
      "{'loss': 3.0414, 'learning_rate': 0.00011966759002770083, 'epoch': 1.42}        \n",
      "{'loss': 2.6652, 'learning_rate': 0.00011939058171745152, 'epoch': 1.43}        \n",
      "{'loss': 2.8769, 'learning_rate': 0.00011911357340720222, 'epoch': 1.43}        \n",
      "{'loss': 2.5004, 'learning_rate': 0.00011883656509695291, 'epoch': 1.43}        \n",
      "{'loss': 3.0134, 'learning_rate': 0.0001185595567867036, 'epoch': 1.44}         \n",
      "{'loss': 2.5632, 'learning_rate': 0.00011828254847645431, 'epoch': 1.44}        \n",
      "{'loss': 2.8183, 'learning_rate': 0.000118005540166205, 'epoch': 1.45}          \n",
      "{'loss': 2.5993, 'learning_rate': 0.00011772853185595569, 'epoch': 1.45}        \n",
      "{'loss': 2.8634, 'learning_rate': 0.00011745152354570639, 'epoch': 1.45}        \n",
      "{'loss': 2.9068, 'learning_rate': 0.00011717451523545706, 'epoch': 1.46}        \n",
      "{'loss': 2.8051, 'learning_rate': 0.00011689750692520776, 'epoch': 1.46}        \n",
      "{'loss': 2.6647, 'learning_rate': 0.00011662049861495845, 'epoch': 1.46}        \n",
      "{'loss': 2.6164, 'learning_rate': 0.00011634349030470914, 'epoch': 1.47}        \n",
      "{'loss': 2.3349, 'learning_rate': 0.00011606648199445984, 'epoch': 1.47}        \n",
      "{'loss': 2.7979, 'learning_rate': 0.00011578947368421053, 'epoch': 1.47}        \n",
      "{'loss': 2.6741, 'learning_rate': 0.00011551246537396124, 'epoch': 1.48}        \n",
      "{'loss': 2.4556, 'learning_rate': 0.00011523545706371193, 'epoch': 1.48}        \n",
      "{'loss': 2.4409, 'learning_rate': 0.0001149584487534626, 'epoch': 1.49}         \n",
      "{'loss': 2.774, 'learning_rate': 0.0001146814404432133, 'epoch': 1.49}          \n",
      "{'loss': 2.5467, 'learning_rate': 0.00011440443213296399, 'epoch': 1.49}        \n",
      "{'loss': 3.1948, 'learning_rate': 0.00011412742382271469, 'epoch': 1.5}         \n",
      "{'loss': 3.0016, 'learning_rate': 0.00011385041551246538, 'epoch': 1.5}         \n",
      "{'loss': 2.8118, 'learning_rate': 0.00011357340720221607, 'epoch': 1.5}         \n",
      "{'loss': 2.8423, 'learning_rate': 0.00011329639889196677, 'epoch': 1.51}        \n",
      "{'loss': 2.7605, 'learning_rate': 0.00011301939058171746, 'epoch': 1.51}        \n",
      "{'loss': 2.9348, 'learning_rate': 0.00011274238227146814, 'epoch': 1.51}        \n",
      "{'loss': 2.6539, 'learning_rate': 0.00011246537396121883, 'epoch': 1.52}        \n",
      "{'loss': 2.9314, 'learning_rate': 0.00011218836565096954, 'epoch': 1.52}        \n",
      "{'loss': 2.405, 'learning_rate': 0.00011191135734072023, 'epoch': 1.53}         \n",
      "{'loss': 2.8841, 'learning_rate': 0.00011163434903047092, 'epoch': 1.53}        \n",
      "{'loss': 2.4738, 'learning_rate': 0.00011135734072022162, 'epoch': 1.53}        \n",
      "{'loss': 2.9847, 'learning_rate': 0.00011108033240997231, 'epoch': 1.54}        \n",
      "{'loss': 2.6701, 'learning_rate': 0.000110803324099723, 'epoch': 1.54}          \n",
      "{'loss': 2.6734, 'learning_rate': 0.0001105263157894737, 'epoch': 1.54}         \n",
      "{'loss': 2.8789, 'learning_rate': 0.00011024930747922437, 'epoch': 1.55}        \n",
      "{'loss': 2.7791, 'learning_rate': 0.00010997229916897507, 'epoch': 1.55}        \n",
      "{'loss': 2.5343, 'learning_rate': 0.00010969529085872576, 'epoch': 1.55}        \n",
      "{'loss': 2.843, 'learning_rate': 0.00010941828254847645, 'epoch': 1.56}         \n",
      "{'loss': 2.6555, 'learning_rate': 0.00010914127423822716, 'epoch': 1.56}        \n",
      "{'loss': 2.5801, 'learning_rate': 0.00010886426592797785, 'epoch': 1.57}        \n",
      "{'loss': 2.5808, 'learning_rate': 0.00010858725761772855, 'epoch': 1.57}        \n",
      "{'loss': 2.6795, 'learning_rate': 0.00010831024930747924, 'epoch': 1.57}        \n",
      "{'loss': 2.6538, 'learning_rate': 0.00010803324099722992, 'epoch': 1.58}        \n",
      "{'loss': 2.9248, 'learning_rate': 0.00010775623268698061, 'epoch': 1.58}        \n",
      "{'loss': 2.946, 'learning_rate': 0.0001074792243767313, 'epoch': 1.58}          \n",
      "{'loss': 2.8592, 'learning_rate': 0.000107202216066482, 'epoch': 1.59}          \n",
      "{'loss': 2.6318, 'learning_rate': 0.00010692520775623269, 'epoch': 1.59}        \n",
      "{'loss': 2.8123, 'learning_rate': 0.00010664819944598338, 'epoch': 1.59}        \n",
      "{'loss': 2.6615, 'learning_rate': 0.00010637119113573409, 'epoch': 1.6}         \n",
      "{'loss': 2.6308, 'learning_rate': 0.00010609418282548478, 'epoch': 1.6}         \n",
      "{'loss': 2.9102, 'learning_rate': 0.00010581717451523547, 'epoch': 1.61}        \n",
      "{'loss': 2.6578, 'learning_rate': 0.00010554016620498614, 'epoch': 1.61}        \n",
      "{'loss': 2.5604, 'learning_rate': 0.00010526315789473685, 'epoch': 1.61}        \n",
      "{'loss': 2.6599, 'learning_rate': 0.00010498614958448754, 'epoch': 1.62}        \n",
      "{'loss': 2.6846, 'learning_rate': 0.00010470914127423823, 'epoch': 1.62}        \n",
      "{'loss': 2.6847, 'learning_rate': 0.00010443213296398893, 'epoch': 1.62}        \n",
      "{'loss': 2.6061, 'learning_rate': 0.00010415512465373962, 'epoch': 1.63}        \n",
      "{'loss': 2.8705, 'learning_rate': 0.00010387811634349031, 'epoch': 1.63}        \n",
      "{'loss': 2.7902, 'learning_rate': 0.00010360110803324102, 'epoch': 1.64}        \n",
      "{'loss': 2.3295, 'learning_rate': 0.00010332409972299168, 'epoch': 1.64}        \n",
      "{'loss': 2.8137, 'learning_rate': 0.00010304709141274238, 'epoch': 1.64}        \n",
      "{'loss': 3.2132, 'learning_rate': 0.00010277008310249307, 'epoch': 1.65}        \n",
      "{'loss': 2.5788, 'learning_rate': 0.00010249307479224376, 'epoch': 1.65}        \n",
      "{'loss': 2.629, 'learning_rate': 0.00010221606648199447, 'epoch': 1.65}         \n",
      "{'loss': 2.4119, 'learning_rate': 0.00010193905817174516, 'epoch': 1.66}        \n",
      "{'loss': 2.8367, 'learning_rate': 0.00010166204986149586, 'epoch': 1.66}        \n",
      "{'loss': 2.6347, 'learning_rate': 0.00010138504155124655, 'epoch': 1.66}        \n",
      "{'loss': 2.7092, 'learning_rate': 0.00010110803324099723, 'epoch': 1.67}        \n",
      "{'loss': 2.6922, 'learning_rate': 0.00010083102493074792, 'epoch': 1.67}        \n",
      "{'loss': 2.6683, 'learning_rate': 0.00010055401662049861, 'epoch': 1.68}        \n",
      "{'loss': 2.5938, 'learning_rate': 0.00010027700831024931, 'epoch': 1.68}        \n",
      "{'loss': 3.1796, 'learning_rate': 0.0001, 'epoch': 1.68}                        \n",
      "{'loss': 2.515, 'learning_rate': 9.97229916897507e-05, 'epoch': 1.69}           \n",
      "{'loss': 2.889, 'learning_rate': 9.94459833795014e-05, 'epoch': 1.69}           \n",
      "{'loss': 2.5619, 'learning_rate': 9.916897506925208e-05, 'epoch': 1.69}         \n",
      "{'loss': 2.7653, 'learning_rate': 9.889196675900278e-05, 'epoch': 1.7}          \n",
      "{'loss': 2.8784, 'learning_rate': 9.861495844875347e-05, 'epoch': 1.7}          \n",
      "{'loss': 2.8262, 'learning_rate': 9.833795013850416e-05, 'epoch': 1.7}          \n",
      "{'loss': 2.3818, 'learning_rate': 9.806094182825485e-05, 'epoch': 1.71}         \n",
      "{'loss': 2.8654, 'learning_rate': 9.778393351800554e-05, 'epoch': 1.71}         \n",
      "{'loss': 2.7399, 'learning_rate': 9.750692520775624e-05, 'epoch': 1.72}         \n",
      "{'loss': 2.8779, 'learning_rate': 9.722991689750694e-05, 'epoch': 1.72}         \n",
      "{'loss': 2.8474, 'learning_rate': 9.695290858725761e-05, 'epoch': 1.72}         \n",
      "{'loss': 2.738, 'learning_rate': 9.667590027700832e-05, 'epoch': 1.73}          \n",
      "{'loss': 2.7667, 'learning_rate': 9.6398891966759e-05, 'epoch': 1.73}           \n",
      "{'loss': 2.7871, 'learning_rate': 9.612188365650971e-05, 'epoch': 1.73}         \n",
      "{'loss': 2.9359, 'learning_rate': 9.584487534626039e-05, 'epoch': 1.74}         \n",
      "{'loss': 2.9027, 'learning_rate': 9.556786703601108e-05, 'epoch': 1.74}         \n",
      "{'loss': 2.7713, 'learning_rate': 9.529085872576178e-05, 'epoch': 1.74}         \n",
      "{'loss': 2.6837, 'learning_rate': 9.501385041551247e-05, 'epoch': 1.75}         \n",
      "{'loss': 2.8722, 'learning_rate': 9.473684210526316e-05, 'epoch': 1.75}         \n",
      "{'loss': 2.5367, 'learning_rate': 9.445983379501385e-05, 'epoch': 1.76}         \n",
      "{'loss': 2.6289, 'learning_rate': 9.418282548476454e-05, 'epoch': 1.76}         \n",
      "{'loss': 2.752, 'learning_rate': 9.390581717451525e-05, 'epoch': 1.76}          \n",
      "{'loss': 2.7444, 'learning_rate': 9.362880886426594e-05, 'epoch': 1.77}         \n",
      "{'loss': 2.6541, 'learning_rate': 9.335180055401663e-05, 'epoch': 1.77}         \n",
      "{'loss': 2.8663, 'learning_rate': 9.307479224376732e-05, 'epoch': 1.77}         \n",
      "{'loss': 2.6664, 'learning_rate': 9.279778393351801e-05, 'epoch': 1.78}         \n",
      "{'loss': 2.7903, 'learning_rate': 9.252077562326871e-05, 'epoch': 1.78}         \n",
      "{'loss': 2.6619, 'learning_rate': 9.224376731301939e-05, 'epoch': 1.78}         \n",
      "{'loss': 3.152, 'learning_rate': 9.196675900277009e-05, 'epoch': 1.79}          \n",
      "{'loss': 2.9435, 'learning_rate': 9.168975069252078e-05, 'epoch': 1.79}         \n",
      "{'loss': 2.4054, 'learning_rate': 9.141274238227147e-05, 'epoch': 1.8}          \n",
      "{'loss': 2.9798, 'learning_rate': 9.113573407202216e-05, 'epoch': 1.8}          \n",
      "{'loss': 2.8829, 'learning_rate': 9.085872576177285e-05, 'epoch': 1.8}          \n",
      "{'loss': 3.1133, 'learning_rate': 9.058171745152356e-05, 'epoch': 1.81}         \n",
      "{'loss': 2.3862, 'learning_rate': 9.030470914127425e-05, 'epoch': 1.81}         \n",
      "{'loss': 2.876, 'learning_rate': 9.002770083102492e-05, 'epoch': 1.81}          \n",
      "{'loss': 2.6786, 'learning_rate': 8.975069252077563e-05, 'epoch': 1.82}         \n",
      "{'loss': 2.7265, 'learning_rate': 8.947368421052632e-05, 'epoch': 1.82}         \n",
      "{'loss': 2.8073, 'learning_rate': 8.919667590027702e-05, 'epoch': 1.82}         \n",
      "{'loss': 2.1877, 'learning_rate': 8.89196675900277e-05, 'epoch': 1.83}          \n",
      "{'loss': 2.8111, 'learning_rate': 8.864265927977839e-05, 'epoch': 1.83}         \n",
      "{'loss': 3.0495, 'learning_rate': 8.83656509695291e-05, 'epoch': 1.84}          \n",
      "{'loss': 2.5664, 'learning_rate': 8.808864265927978e-05, 'epoch': 1.84}         \n",
      "{'loss': 2.8628, 'learning_rate': 8.781163434903047e-05, 'epoch': 1.84}         \n",
      "{'loss': 2.7119, 'learning_rate': 8.753462603878116e-05, 'epoch': 1.85}         \n",
      "{'loss': 2.6447, 'learning_rate': 8.725761772853185e-05, 'epoch': 1.85}         \n",
      "{'loss': 2.8664, 'learning_rate': 8.698060941828256e-05, 'epoch': 1.85}         \n",
      "{'loss': 2.7027, 'learning_rate': 8.670360110803325e-05, 'epoch': 1.86}         \n",
      "{'loss': 2.5187, 'learning_rate': 8.642659279778394e-05, 'epoch': 1.86}         \n",
      "{'loss': 2.5081, 'learning_rate': 8.614958448753463e-05, 'epoch': 1.86}         \n",
      "{'loss': 2.4862, 'learning_rate': 8.587257617728532e-05, 'epoch': 1.87}         \n",
      "{'loss': 2.5825, 'learning_rate': 8.559556786703602e-05, 'epoch': 1.87}         \n",
      "{'loss': 2.7662, 'learning_rate': 8.53185595567867e-05, 'epoch': 1.88}          \n",
      "{'loss': 2.6755, 'learning_rate': 8.50415512465374e-05, 'epoch': 1.88}          \n",
      "{'loss': 2.7887, 'learning_rate': 8.47645429362881e-05, 'epoch': 1.88}          \n",
      "{'loss': 2.9911, 'learning_rate': 8.448753462603879e-05, 'epoch': 1.89}         \n",
      "{'loss': 2.5541, 'learning_rate': 8.421052631578948e-05, 'epoch': 1.89}         \n",
      "{'loss': 2.6465, 'learning_rate': 8.393351800554017e-05, 'epoch': 1.89}         \n",
      "{'loss': 2.5148, 'learning_rate': 8.365650969529087e-05, 'epoch': 1.9}          \n",
      "{'loss': 2.4811, 'learning_rate': 8.337950138504156e-05, 'epoch': 1.9}          \n",
      "{'loss': 2.998, 'learning_rate': 8.310249307479224e-05, 'epoch': 1.91}          \n",
      "{'loss': 3.014, 'learning_rate': 8.282548476454294e-05, 'epoch': 1.91}          \n",
      "{'loss': 2.7376, 'learning_rate': 8.254847645429363e-05, 'epoch': 1.91}         \n",
      "{'loss': 2.8848, 'learning_rate': 8.227146814404432e-05, 'epoch': 1.92}         \n",
      "{'loss': 2.5092, 'learning_rate': 8.199445983379503e-05, 'epoch': 1.92}         \n",
      "{'loss': 2.7706, 'learning_rate': 8.17174515235457e-05, 'epoch': 1.92}          \n",
      "{'loss': 2.8845, 'learning_rate': 8.14404432132964e-05, 'epoch': 1.93}          \n",
      "{'loss': 2.4966, 'learning_rate': 8.11634349030471e-05, 'epoch': 1.93}          \n",
      "{'loss': 2.551, 'learning_rate': 8.088642659279779e-05, 'epoch': 1.93}          \n",
      "{'loss': 2.6348, 'learning_rate': 8.060941828254848e-05, 'epoch': 1.94}         \n",
      "{'loss': 2.6043, 'learning_rate': 8.033240997229917e-05, 'epoch': 1.94}         \n",
      "{'loss': 2.5779, 'learning_rate': 8.005540166204987e-05, 'epoch': 1.95}         \n",
      "{'loss': 2.6503, 'learning_rate': 7.977839335180056e-05, 'epoch': 1.95}         \n",
      "{'loss': 2.8901, 'learning_rate': 7.950138504155125e-05, 'epoch': 1.95}         \n",
      "{'loss': 2.642, 'learning_rate': 7.922437673130194e-05, 'epoch': 1.96}          \n",
      "{'loss': 2.4532, 'learning_rate': 7.894736842105263e-05, 'epoch': 1.96}         \n",
      "{'loss': 2.6297, 'learning_rate': 7.867036011080334e-05, 'epoch': 1.96}         \n",
      "{'loss': 2.7872, 'learning_rate': 7.839335180055401e-05, 'epoch': 1.97}         \n",
      "{'loss': 3.0265, 'learning_rate': 7.811634349030472e-05, 'epoch': 1.97}         \n",
      "{'loss': 2.6481, 'learning_rate': 7.783933518005541e-05, 'epoch': 1.97}         \n",
      "{'loss': 3.1212, 'learning_rate': 7.75623268698061e-05, 'epoch': 1.98}          \n",
      "{'loss': 3.0564, 'learning_rate': 7.728531855955679e-05, 'epoch': 1.98}         \n",
      "{'loss': 2.8313, 'learning_rate': 7.700831024930748e-05, 'epoch': 1.99}         \n",
      "{'loss': 2.7656, 'learning_rate': 7.673130193905818e-05, 'epoch': 1.99}         \n",
      "{'loss': 2.3943, 'learning_rate': 7.645429362880887e-05, 'epoch': 1.99}         \n",
      "{'loss': 2.566, 'learning_rate': 7.617728531855956e-05, 'epoch': 2.0}           \n",
      "{'loss': 2.3448, 'learning_rate': 7.590027700831025e-05, 'epoch': 2.0}          \n",
      "{'loss': 2.5839, 'learning_rate': 7.562326869806094e-05, 'epoch': 2.0}          \n",
      "{'loss': 2.5039, 'learning_rate': 7.534626038781163e-05, 'epoch': 2.01}         \n",
      "{'loss': 2.4794, 'learning_rate': 7.506925207756234e-05, 'epoch': 2.01}         \n",
      "{'loss': 2.6386, 'learning_rate': 7.479224376731301e-05, 'epoch': 2.01}         \n",
      "{'loss': 2.5724, 'learning_rate': 7.451523545706372e-05, 'epoch': 2.02}         \n",
      "{'loss': 2.7962, 'learning_rate': 7.423822714681441e-05, 'epoch': 2.02}         \n",
      "{'loss': 2.7048, 'learning_rate': 7.39612188365651e-05, 'epoch': 2.03}          \n",
      "{'loss': 2.6599, 'learning_rate': 7.368421052631579e-05, 'epoch': 2.03}         \n",
      "{'loss': 2.5338, 'learning_rate': 7.340720221606648e-05, 'epoch': 2.03}         \n",
      "{'loss': 2.4668, 'learning_rate': 7.313019390581718e-05, 'epoch': 2.04}         \n",
      "{'loss': 2.4666, 'learning_rate': 7.285318559556787e-05, 'epoch': 2.04}         \n",
      "{'loss': 2.7735, 'learning_rate': 7.257617728531856e-05, 'epoch': 2.04}         \n",
      "{'loss': 2.5964, 'learning_rate': 7.229916897506925e-05, 'epoch': 2.05}         \n",
      "{'loss': 2.5965, 'learning_rate': 7.202216066481994e-05, 'epoch': 2.05}         \n",
      "{'loss': 2.9012, 'learning_rate': 7.174515235457065e-05, 'epoch': 2.05}         \n",
      "{'loss': 2.3802, 'learning_rate': 7.146814404432133e-05, 'epoch': 2.06}         \n",
      "{'loss': 2.7531, 'learning_rate': 7.119113573407203e-05, 'epoch': 2.06}         \n",
      "{'loss': 2.8357, 'learning_rate': 7.091412742382272e-05, 'epoch': 2.07}         \n",
      "{'loss': 2.5873, 'learning_rate': 7.063711911357341e-05, 'epoch': 2.07}         \n",
      "{'loss': 2.5276, 'learning_rate': 7.036011080332411e-05, 'epoch': 2.07}         \n",
      "{'loss': 2.8305, 'learning_rate': 7.008310249307479e-05, 'epoch': 2.08}         \n",
      "{'loss': 2.3762, 'learning_rate': 6.980609418282548e-05, 'epoch': 2.08}         \n",
      "{'loss': 2.8547, 'learning_rate': 6.952908587257619e-05, 'epoch': 2.08}         \n",
      "{'loss': 2.8288, 'learning_rate': 6.925207756232688e-05, 'epoch': 2.09}         \n",
      "{'loss': 2.7847, 'learning_rate': 6.897506925207757e-05, 'epoch': 2.09}         \n",
      "{'loss': 2.2932, 'learning_rate': 6.869806094182826e-05, 'epoch': 2.09}         \n",
      "{'loss': 2.6336, 'learning_rate': 6.842105263157895e-05, 'epoch': 2.1}          \n",
      "{'loss': 2.701, 'learning_rate': 6.814404432132965e-05, 'epoch': 2.1}           \n",
      "{'loss': 2.8401, 'learning_rate': 6.786703601108033e-05, 'epoch': 2.11}         \n",
      "{'loss': 2.7769, 'learning_rate': 6.759002770083103e-05, 'epoch': 2.11}         \n",
      "{'loss': 2.665, 'learning_rate': 6.731301939058172e-05, 'epoch': 2.11}          \n",
      "{'loss': 2.7086, 'learning_rate': 6.703601108033241e-05, 'epoch': 2.12}         \n",
      "{'loss': 2.8312, 'learning_rate': 6.67590027700831e-05, 'epoch': 2.12}          \n",
      "{'loss': 2.6652, 'learning_rate': 6.648199445983379e-05, 'epoch': 2.12}         \n",
      "{'loss': 2.6755, 'learning_rate': 6.62049861495845e-05, 'epoch': 2.13}          \n",
      "{'loss': 2.8723, 'learning_rate': 6.592797783933519e-05, 'epoch': 2.13}         \n",
      "{'loss': 2.5988, 'learning_rate': 6.565096952908588e-05, 'epoch': 2.14}         \n",
      "{'loss': 2.8149, 'learning_rate': 6.537396121883657e-05, 'epoch': 2.14}         \n",
      "{'loss': 2.6215, 'learning_rate': 6.509695290858726e-05, 'epoch': 2.14}         \n",
      "{'loss': 2.793, 'learning_rate': 6.481994459833796e-05, 'epoch': 2.15}          \n",
      "{'loss': 2.7062, 'learning_rate': 6.454293628808865e-05, 'epoch': 2.15}         \n",
      "{'loss': 3.0939, 'learning_rate': 6.426592797783934e-05, 'epoch': 2.15}         \n",
      "{'loss': 3.1068, 'learning_rate': 6.398891966759003e-05, 'epoch': 2.16}         \n",
      "{'loss': 2.5783, 'learning_rate': 6.371191135734072e-05, 'epoch': 2.16}         \n",
      "{'loss': 2.6884, 'learning_rate': 6.343490304709143e-05, 'epoch': 2.16}         \n",
      "{'loss': 3.0118, 'learning_rate': 6.31578947368421e-05, 'epoch': 2.17}          \n",
      "{'loss': 2.4645, 'learning_rate': 6.28808864265928e-05, 'epoch': 2.17}          \n",
      "{'loss': 2.4176, 'learning_rate': 6.26038781163435e-05, 'epoch': 2.18}          \n",
      "{'loss': 2.8688, 'learning_rate': 6.232686980609419e-05, 'epoch': 2.18}         \n",
      "{'loss': 2.832, 'learning_rate': 6.204986149584488e-05, 'epoch': 2.18}          \n",
      "{'loss': 2.3629, 'learning_rate': 6.177285318559557e-05, 'epoch': 2.19}         \n",
      "{'loss': 2.4716, 'learning_rate': 6.149584487534626e-05, 'epoch': 2.19}         \n",
      "{'loss': 2.1692, 'learning_rate': 6.121883656509696e-05, 'epoch': 2.19}         \n",
      "{'loss': 2.865, 'learning_rate': 6.0941828254847646e-05, 'epoch': 2.2}          \n",
      "{'loss': 2.7192, 'learning_rate': 6.0664819944598337e-05, 'epoch': 2.2}         \n",
      "{'loss': 2.5961, 'learning_rate': 6.0387811634349034e-05, 'epoch': 2.2}         \n",
      "{'loss': 2.6405, 'learning_rate': 6.011080332409973e-05, 'epoch': 2.21}         \n",
      "{'loss': 2.4841, 'learning_rate': 5.9833795013850414e-05, 'epoch': 2.21}        \n",
      "{'loss': 2.6506, 'learning_rate': 5.955678670360111e-05, 'epoch': 2.22}         \n",
      "{'loss': 2.8225, 'learning_rate': 5.92797783933518e-05, 'epoch': 2.22}          \n",
      "{'loss': 2.8626, 'learning_rate': 5.90027700831025e-05, 'epoch': 2.22}          \n",
      "{'loss': 2.8477, 'learning_rate': 5.8725761772853196e-05, 'epoch': 2.23}        \n",
      "{'loss': 3.0628, 'learning_rate': 5.844875346260388e-05, 'epoch': 2.23}         \n",
      "{'loss': 2.6317, 'learning_rate': 5.817174515235457e-05, 'epoch': 2.23}         \n",
      "{'loss': 2.6074, 'learning_rate': 5.789473684210527e-05, 'epoch': 2.24}         \n",
      "{'loss': 2.6854, 'learning_rate': 5.7617728531855964e-05, 'epoch': 2.24}        \n",
      "{'loss': 2.8291, 'learning_rate': 5.734072022160665e-05, 'epoch': 2.24}         \n",
      "{'loss': 2.3194, 'learning_rate': 5.7063711911357345e-05, 'epoch': 2.25}        \n",
      "{'loss': 2.6253, 'learning_rate': 5.6786703601108035e-05, 'epoch': 2.25}        \n",
      "{'loss': 2.7811, 'learning_rate': 5.650969529085873e-05, 'epoch': 2.26}         \n",
      "{'loss': 2.6776, 'learning_rate': 5.6232686980609416e-05, 'epoch': 2.26}        \n",
      "{'loss': 2.6648, 'learning_rate': 5.595567867036011e-05, 'epoch': 2.26}         \n",
      "{'loss': 2.5256, 'learning_rate': 5.567867036011081e-05, 'epoch': 2.27}         \n",
      "{'loss': 2.7468, 'learning_rate': 5.54016620498615e-05, 'epoch': 2.27}          \n",
      "{'loss': 2.6294, 'learning_rate': 5.5124653739612184e-05, 'epoch': 2.27}        \n",
      "{'loss': 2.8733, 'learning_rate': 5.484764542936288e-05, 'epoch': 2.28}         \n",
      "{'loss': 2.5858, 'learning_rate': 5.457063711911358e-05, 'epoch': 2.28}         \n",
      "{'loss': 2.7823, 'learning_rate': 5.4293628808864275e-05, 'epoch': 2.28}        \n",
      "{'loss': 2.8459, 'learning_rate': 5.401662049861496e-05, 'epoch': 2.29}         \n",
      "{'loss': 3.0541, 'learning_rate': 5.373961218836565e-05, 'epoch': 2.29}         \n",
      "{'loss': 3.0167, 'learning_rate': 5.3462603878116346e-05, 'epoch': 2.3}         \n",
      "{'loss': 2.538, 'learning_rate': 5.318559556786704e-05, 'epoch': 2.3}           \n",
      "{'loss': 2.6757, 'learning_rate': 5.2908587257617734e-05, 'epoch': 2.3}         \n",
      "{'loss': 2.6938, 'learning_rate': 5.2631578947368424e-05, 'epoch': 2.31}        \n",
      "{'loss': 2.6345, 'learning_rate': 5.2354570637119114e-05, 'epoch': 2.31}        \n",
      "{'loss': 2.9175, 'learning_rate': 5.207756232686981e-05, 'epoch': 2.31}         \n",
      "{'loss': 2.9641, 'learning_rate': 5.180055401662051e-05, 'epoch': 2.32}         \n",
      "{'loss': 2.7462, 'learning_rate': 5.152354570637119e-05, 'epoch': 2.32}         \n",
      "{'loss': 3.1331, 'learning_rate': 5.124653739612188e-05, 'epoch': 2.32}         \n",
      "{'loss': 2.8517, 'learning_rate': 5.096952908587258e-05, 'epoch': 2.33}         \n",
      "{'loss': 2.9784, 'learning_rate': 5.0692520775623277e-05, 'epoch': 2.33}        \n",
      "{'loss': 2.636, 'learning_rate': 5.041551246537396e-05, 'epoch': 2.34}          \n",
      "{'loss': 2.6957, 'learning_rate': 5.013850415512466e-05, 'epoch': 2.34}         \n",
      "{'loss': 2.9641, 'learning_rate': 4.986149584487535e-05, 'epoch': 2.34}         \n",
      "{'loss': 2.5723, 'learning_rate': 4.958448753462604e-05, 'epoch': 2.35}         \n",
      "{'loss': 3.1856, 'learning_rate': 4.9307479224376735e-05, 'epoch': 2.35}        \n",
      "{'loss': 2.6186, 'learning_rate': 4.9030470914127425e-05, 'epoch': 2.35}        \n",
      "{'loss': 2.5553, 'learning_rate': 4.875346260387812e-05, 'epoch': 2.36}         \n",
      "{'loss': 2.6501, 'learning_rate': 4.8476454293628806e-05, 'epoch': 2.36}        \n",
      "{'loss': 2.924, 'learning_rate': 4.81994459833795e-05, 'epoch': 2.36}           \n",
      "{'loss': 2.4968, 'learning_rate': 4.7922437673130193e-05, 'epoch': 2.37}        \n",
      "{'loss': 2.5232, 'learning_rate': 4.764542936288089e-05, 'epoch': 2.37}         \n",
      "{'loss': 3.1412, 'learning_rate': 4.736842105263158e-05, 'epoch': 2.38}         \n",
      "{'loss': 2.8039, 'learning_rate': 4.709141274238227e-05, 'epoch': 2.38}         \n",
      "{'loss': 2.596, 'learning_rate': 4.681440443213297e-05, 'epoch': 2.38}          \n",
      "{'loss': 2.8108, 'learning_rate': 4.653739612188366e-05, 'epoch': 2.39}         \n",
      "{'loss': 2.9355, 'learning_rate': 4.6260387811634356e-05, 'epoch': 2.39}        \n",
      "{'loss': 2.9046, 'learning_rate': 4.5983379501385046e-05, 'epoch': 2.39}        \n",
      "{'loss': 2.5485, 'learning_rate': 4.5706371191135736e-05, 'epoch': 2.4}         \n",
      "{'loss': 2.4659, 'learning_rate': 4.542936288088643e-05, 'epoch': 2.4}          \n",
      "{'loss': 2.7307, 'learning_rate': 4.5152354570637124e-05, 'epoch': 2.41}        \n",
      "{'loss': 2.8461, 'learning_rate': 4.4875346260387814e-05, 'epoch': 2.41}        \n",
      "{'loss': 2.866, 'learning_rate': 4.459833795013851e-05, 'epoch': 2.41}          \n",
      "{'loss': 2.5946, 'learning_rate': 4.4321329639889195e-05, 'epoch': 2.42}        \n",
      "{'loss': 2.6891, 'learning_rate': 4.404432132963989e-05, 'epoch': 2.42}         \n",
      "{'loss': 3.0043, 'learning_rate': 4.376731301939058e-05, 'epoch': 2.42}         \n",
      "{'loss': 2.872, 'learning_rate': 4.349030470914128e-05, 'epoch': 2.43}          \n",
      "{'loss': 2.6667, 'learning_rate': 4.321329639889197e-05, 'epoch': 2.43}         \n",
      "{'loss': 3.0637, 'learning_rate': 4.293628808864266e-05, 'epoch': 2.43}         \n",
      "{'loss': 2.5562, 'learning_rate': 4.265927977839335e-05, 'epoch': 2.44}         \n",
      "{'loss': 2.3892, 'learning_rate': 4.238227146814405e-05, 'epoch': 2.44}         \n",
      "{'loss': 2.4921, 'learning_rate': 4.210526315789474e-05, 'epoch': 2.45}         \n",
      "{'loss': 2.5554, 'learning_rate': 4.1828254847645435e-05, 'epoch': 2.45}        \n",
      "{'loss': 2.8121, 'learning_rate': 4.155124653739612e-05, 'epoch': 2.45}         \n",
      "{'loss': 2.9719, 'learning_rate': 4.1274238227146816e-05, 'epoch': 2.46}        \n",
      "{'loss': 2.7184, 'learning_rate': 4.099722991689751e-05, 'epoch': 2.46}         \n",
      "{'loss': 2.4935, 'learning_rate': 4.07202216066482e-05, 'epoch': 2.46}          \n",
      "{'loss': 2.3041, 'learning_rate': 4.044321329639889e-05, 'epoch': 2.47}         \n",
      "{'loss': 2.5628, 'learning_rate': 4.0166204986149584e-05, 'epoch': 2.47}        \n",
      "{'loss': 3.2562, 'learning_rate': 3.988919667590028e-05, 'epoch': 2.47}         \n",
      "{'loss': 2.6166, 'learning_rate': 3.961218836565097e-05, 'epoch': 2.48}         \n",
      "{'loss': 2.539, 'learning_rate': 3.933518005540167e-05, 'epoch': 2.48}          \n",
      "{'loss': 2.6087, 'learning_rate': 3.905817174515236e-05, 'epoch': 2.49}         \n",
      "{'loss': 3.1234, 'learning_rate': 3.878116343490305e-05, 'epoch': 2.49}         \n",
      "{'loss': 2.4773, 'learning_rate': 3.850415512465374e-05, 'epoch': 2.49}         \n",
      "{'loss': 2.8219, 'learning_rate': 3.8227146814404436e-05, 'epoch': 2.5}         \n",
      "{'loss': 2.5633, 'learning_rate': 3.795013850415513e-05, 'epoch': 2.5}          \n",
      "{'loss': 2.6201, 'learning_rate': 3.767313019390582e-05, 'epoch': 2.5}          \n",
      "{'loss': 2.8617, 'learning_rate': 3.739612188365651e-05, 'epoch': 2.51}         \n",
      "{'loss': 2.5739, 'learning_rate': 3.7119113573407204e-05, 'epoch': 2.51}        \n",
      "{'loss': 2.6496, 'learning_rate': 3.6842105263157895e-05, 'epoch': 2.51}        \n",
      "{'loss': 2.6221, 'learning_rate': 3.656509695290859e-05, 'epoch': 2.52}         \n",
      "{'loss': 2.637, 'learning_rate': 3.628808864265928e-05, 'epoch': 2.52}          \n",
      "{'loss': 2.5806, 'learning_rate': 3.601108033240997e-05, 'epoch': 2.53}         \n",
      "{'loss': 2.9319, 'learning_rate': 3.573407202216066e-05, 'epoch': 2.53}         \n",
      "{'loss': 2.7207, 'learning_rate': 3.545706371191136e-05, 'epoch': 2.53}         \n",
      "{'loss': 2.684, 'learning_rate': 3.518005540166206e-05, 'epoch': 2.54}          \n",
      "{'loss': 2.6722, 'learning_rate': 3.490304709141274e-05, 'epoch': 2.54}         \n",
      "{'loss': 2.5082, 'learning_rate': 3.462603878116344e-05, 'epoch': 2.54}         \n",
      "{'loss': 2.421, 'learning_rate': 3.434903047091413e-05, 'epoch': 2.55}          \n",
      "{'loss': 2.811, 'learning_rate': 3.4072022160664825e-05, 'epoch': 2.55}         \n",
      "{'loss': 2.619, 'learning_rate': 3.3795013850415515e-05, 'epoch': 2.55}         \n",
      "{'loss': 2.5138, 'learning_rate': 3.3518005540166206e-05, 'epoch': 2.56}        \n",
      "{'loss': 2.5184, 'learning_rate': 3.3240997229916896e-05, 'epoch': 2.56}        \n",
      "{'loss': 2.7575, 'learning_rate': 3.296398891966759e-05, 'epoch': 2.57}         \n",
      "{'loss': 2.5794, 'learning_rate': 3.2686980609418284e-05, 'epoch': 2.57}        \n",
      "{'loss': 3.2656, 'learning_rate': 3.240997229916898e-05, 'epoch': 2.57}         \n",
      "{'loss': 2.3882, 'learning_rate': 3.213296398891967e-05, 'epoch': 2.58}         \n",
      "{'loss': 2.8044, 'learning_rate': 3.185595567867036e-05, 'epoch': 2.58}         \n",
      "{'loss': 2.9903, 'learning_rate': 3.157894736842105e-05, 'epoch': 2.58}         \n",
      "{'loss': 2.626, 'learning_rate': 3.130193905817175e-05, 'epoch': 2.59}          \n",
      "{'loss': 2.8401, 'learning_rate': 3.102493074792244e-05, 'epoch': 2.59}         \n",
      "{'loss': 2.7171, 'learning_rate': 3.074792243767313e-05, 'epoch': 2.59}         \n",
      "{'loss': 2.8164, 'learning_rate': 3.0470914127423823e-05, 'epoch': 2.6}         \n",
      "{'loss': 2.6495, 'learning_rate': 3.0193905817174517e-05, 'epoch': 2.6}         \n",
      "{'loss': 2.7534, 'learning_rate': 2.9916897506925207e-05, 'epoch': 2.61}        \n",
      "{'loss': 2.4513, 'learning_rate': 2.96398891966759e-05, 'epoch': 2.61}          \n",
      "{'loss': 2.502, 'learning_rate': 2.9362880886426598e-05, 'epoch': 2.61}         \n",
      "{'loss': 2.7663, 'learning_rate': 2.9085872576177285e-05, 'epoch': 2.62}        \n",
      "{'loss': 2.8525, 'learning_rate': 2.8808864265927982e-05, 'epoch': 2.62}        \n",
      "{'loss': 2.5263, 'learning_rate': 2.8531855955678672e-05, 'epoch': 2.62}        \n",
      "{'loss': 2.096, 'learning_rate': 2.8254847645429366e-05, 'epoch': 2.63}         \n",
      "{'loss': 2.7062, 'learning_rate': 2.7977839335180056e-05, 'epoch': 2.63}        \n",
      "{'loss': 2.8376, 'learning_rate': 2.770083102493075e-05, 'epoch': 2.64}         \n",
      "{'loss': 2.3814, 'learning_rate': 2.742382271468144e-05, 'epoch': 2.64}         \n",
      "{'loss': 2.6423, 'learning_rate': 2.7146814404432138e-05, 'epoch': 2.64}        \n",
      "{'loss': 2.651, 'learning_rate': 2.6869806094182825e-05, 'epoch': 2.65}         \n",
      "{'loss': 2.5416, 'learning_rate': 2.659279778393352e-05, 'epoch': 2.65}         \n",
      "{'loss': 2.8021, 'learning_rate': 2.6315789473684212e-05, 'epoch': 2.65}        \n",
      "{'loss': 2.6343, 'learning_rate': 2.6038781163434906e-05, 'epoch': 2.66}        \n",
      "{'loss': 2.348, 'learning_rate': 2.5761772853185596e-05, 'epoch': 2.66}         \n",
      "{'loss': 2.917, 'learning_rate': 2.548476454293629e-05, 'epoch': 2.66}          \n",
      "{'loss': 2.4465, 'learning_rate': 2.520775623268698e-05, 'epoch': 2.67}         \n",
      "{'loss': 2.4396, 'learning_rate': 2.4930747922437674e-05, 'epoch': 2.67}        \n",
      "{'loss': 3.0342, 'learning_rate': 2.4653739612188367e-05, 'epoch': 2.68}        \n",
      "{'loss': 2.7602, 'learning_rate': 2.437673130193906e-05, 'epoch': 2.68}         \n",
      "{'loss': 2.5351, 'learning_rate': 2.409972299168975e-05, 'epoch': 2.68}         \n",
      "{'loss': 2.8852, 'learning_rate': 2.3822714681440445e-05, 'epoch': 2.69}        \n",
      "{'loss': 2.4378, 'learning_rate': 2.3545706371191136e-05, 'epoch': 2.69}        \n",
      "{'loss': 3.309, 'learning_rate': 2.326869806094183e-05, 'epoch': 2.69}          \n",
      "{'loss': 2.5523, 'learning_rate': 2.2991689750692523e-05, 'epoch': 2.7}         \n",
      "{'loss': 2.7343, 'learning_rate': 2.2714681440443213e-05, 'epoch': 2.7}         \n",
      "{'loss': 2.7466, 'learning_rate': 2.2437673130193907e-05, 'epoch': 2.7}         \n",
      "{'loss': 2.7634, 'learning_rate': 2.2160664819944597e-05, 'epoch': 2.71}        \n",
      "{'loss': 2.8394, 'learning_rate': 2.188365650969529e-05, 'epoch': 2.71}         \n",
      "{'loss': 2.7479, 'learning_rate': 2.1606648199445985e-05, 'epoch': 2.72}        \n",
      "{'loss': 2.6965, 'learning_rate': 2.1329639889196675e-05, 'epoch': 2.72}        \n",
      "{'loss': 2.913, 'learning_rate': 2.105263157894737e-05, 'epoch': 2.72}          \n",
      "{'loss': 2.7881, 'learning_rate': 2.077562326869806e-05, 'epoch': 2.73}         \n",
      "{'loss': 2.9113, 'learning_rate': 2.0498614958448756e-05, 'epoch': 2.73}        \n",
      "{'loss': 2.2748, 'learning_rate': 2.0221606648199447e-05, 'epoch': 2.73}        \n",
      "{'loss': 2.9318, 'learning_rate': 1.994459833795014e-05, 'epoch': 2.74}         \n",
      "{'loss': 2.5577, 'learning_rate': 1.9667590027700834e-05, 'epoch': 2.74}        \n",
      "{'loss': 2.8937, 'learning_rate': 1.9390581717451524e-05, 'epoch': 2.74}        \n",
      "{'loss': 2.8735, 'learning_rate': 1.9113573407202218e-05, 'epoch': 2.75}        \n",
      "{'loss': 2.6283, 'learning_rate': 1.883656509695291e-05, 'epoch': 2.75}         \n",
      "{'loss': 2.6194, 'learning_rate': 1.8559556786703602e-05, 'epoch': 2.76}        \n",
      "{'loss': 2.4035, 'learning_rate': 1.8282548476454296e-05, 'epoch': 2.76}        \n",
      "{'loss': 2.3874, 'learning_rate': 1.8005540166204986e-05, 'epoch': 2.76}        \n",
      "{'loss': 2.7728, 'learning_rate': 1.772853185595568e-05, 'epoch': 2.77}         \n",
      "{'loss': 2.5009, 'learning_rate': 1.745152354570637e-05, 'epoch': 2.77}         \n",
      "{'loss': 2.5222, 'learning_rate': 1.7174515235457064e-05, 'epoch': 2.77}        \n",
      "{'loss': 2.5033, 'learning_rate': 1.6897506925207758e-05, 'epoch': 2.78}        \n",
      "{'loss': 2.8337, 'learning_rate': 1.6620498614958448e-05, 'epoch': 2.78}        \n",
      "{'loss': 2.8949, 'learning_rate': 1.6343490304709142e-05, 'epoch': 2.78}        \n",
      "{'loss': 2.771, 'learning_rate': 1.6066481994459835e-05, 'epoch': 2.79}         \n",
      "{'loss': 3.1525, 'learning_rate': 1.5789473684210526e-05, 'epoch': 2.79}        \n",
      "{'loss': 2.6816, 'learning_rate': 1.551246537396122e-05, 'epoch': 2.8}          \n",
      "{'loss': 2.752, 'learning_rate': 1.5235457063711912e-05, 'epoch': 2.8}          \n",
      "{'loss': 2.8504, 'learning_rate': 1.4958448753462604e-05, 'epoch': 2.8}         \n",
      "{'loss': 2.6303, 'learning_rate': 1.4681440443213299e-05, 'epoch': 2.81}        \n",
      "{'loss': 2.7296, 'learning_rate': 1.4404432132963991e-05, 'epoch': 2.81}        \n",
      "{'loss': 2.5429, 'learning_rate': 1.4127423822714683e-05, 'epoch': 2.81}        \n",
      "{'loss': 2.7683, 'learning_rate': 1.3850415512465375e-05, 'epoch': 2.82}        \n",
      "{'loss': 2.851, 'learning_rate': 1.3573407202216069e-05, 'epoch': 2.82}         \n",
      "{'loss': 2.8781, 'learning_rate': 1.329639889196676e-05, 'epoch': 2.82}         \n",
      "{'loss': 2.2954, 'learning_rate': 1.3019390581717453e-05, 'epoch': 2.83}        \n",
      "{'loss': 2.7702, 'learning_rate': 1.2742382271468145e-05, 'epoch': 2.83}        \n",
      "{'loss': 2.8713, 'learning_rate': 1.2465373961218837e-05, 'epoch': 2.84}        \n",
      "{'loss': 2.8546, 'learning_rate': 1.218836565096953e-05, 'epoch': 2.84}         \n",
      "{'loss': 2.6462, 'learning_rate': 1.1911357340720223e-05, 'epoch': 2.84}        \n",
      "{'loss': 2.9586, 'learning_rate': 1.1634349030470915e-05, 'epoch': 2.85}        \n",
      "{'loss': 2.5431, 'learning_rate': 1.1357340720221607e-05, 'epoch': 2.85}        \n",
      "{'loss': 2.7272, 'learning_rate': 1.1080332409972299e-05, 'epoch': 2.85}        \n",
      "{'loss': 2.7716, 'learning_rate': 1.0803324099722992e-05, 'epoch': 2.86}        \n",
      "{'loss': 2.6465, 'learning_rate': 1.0526315789473684e-05, 'epoch': 2.86}        \n",
      "{'loss': 2.6629, 'learning_rate': 1.0249307479224378e-05, 'epoch': 2.86}        \n",
      "{'loss': 2.5435, 'learning_rate': 9.97229916897507e-06, 'epoch': 2.87}          \n",
      "{'loss': 2.9924, 'learning_rate': 9.695290858725762e-06, 'epoch': 2.87}         \n",
      "{'loss': 2.5842, 'learning_rate': 9.418282548476454e-06, 'epoch': 2.88}         \n",
      "{'loss': 2.8145, 'learning_rate': 9.141274238227148e-06, 'epoch': 2.88}         \n",
      "{'loss': 2.644, 'learning_rate': 8.86426592797784e-06, 'epoch': 2.88}           \n",
      "{'loss': 2.3797, 'learning_rate': 8.587257617728532e-06, 'epoch': 2.89}         \n",
      "{'loss': 2.8349, 'learning_rate': 8.310249307479224e-06, 'epoch': 2.89}         \n",
      "{'loss': 3.1114, 'learning_rate': 8.033240997229918e-06, 'epoch': 2.89}         \n",
      "{'loss': 2.5497, 'learning_rate': 7.75623268698061e-06, 'epoch': 2.9}           \n",
      "{'loss': 2.4192, 'learning_rate': 7.479224376731302e-06, 'epoch': 2.9}          \n",
      "{'loss': 2.9436, 'learning_rate': 7.2022160664819955e-06, 'epoch': 2.91}        \n",
      "{'loss': 2.7367, 'learning_rate': 6.9252077562326875e-06, 'epoch': 2.91}        \n",
      "{'loss': 2.8531, 'learning_rate': 6.64819944598338e-06, 'epoch': 2.91}          \n",
      "{'loss': 2.6844, 'learning_rate': 6.3711911357340724e-06, 'epoch': 2.92}        \n",
      "{'loss': 3.0731, 'learning_rate': 6.094182825484765e-06, 'epoch': 2.92}         \n",
      "{'loss': 3.0622, 'learning_rate': 5.817174515235457e-06, 'epoch': 2.92}         \n",
      "{'loss': 2.8076, 'learning_rate': 5.540166204986149e-06, 'epoch': 2.93}         \n",
      "{'loss': 2.6939, 'learning_rate': 5.263157894736842e-06, 'epoch': 2.93}         \n",
      "{'loss': 2.7399, 'learning_rate': 4.986149584487535e-06, 'epoch': 2.93}         \n",
      "{'loss': 2.6114, 'learning_rate': 4.709141274238227e-06, 'epoch': 2.94}         \n",
      "{'loss': 2.7827, 'learning_rate': 4.43213296398892e-06, 'epoch': 2.94}          \n",
      "{'loss': 2.6318, 'learning_rate': 4.155124653739612e-06, 'epoch': 2.95}         \n",
      "{'loss': 2.5944, 'learning_rate': 3.878116343490305e-06, 'epoch': 2.95}         \n",
      "{'loss': 2.962, 'learning_rate': 3.6011080332409978e-06, 'epoch': 2.95}         \n",
      "{'loss': 2.4381, 'learning_rate': 3.32409972299169e-06, 'epoch': 2.96}          \n",
      "{'loss': 2.6283, 'learning_rate': 3.0470914127423827e-06, 'epoch': 2.96}        \n",
      "{'loss': 2.6303, 'learning_rate': 2.7700831024930747e-06, 'epoch': 2.96}        \n",
      "{'loss': 2.8194, 'learning_rate': 2.4930747922437675e-06, 'epoch': 2.97}        \n",
      "{'loss': 2.6282, 'learning_rate': 2.21606648199446e-06, 'epoch': 2.97}          \n",
      "{'loss': 2.5816, 'learning_rate': 1.9390581717451524e-06, 'epoch': 2.97}        \n",
      "{'loss': 2.3745, 'learning_rate': 1.662049861495845e-06, 'epoch': 2.98}         \n",
      "{'loss': 2.7519, 'learning_rate': 1.3850415512465373e-06, 'epoch': 2.98}        \n",
      "{'loss': 2.6396, 'learning_rate': 1.10803324099723e-06, 'epoch': 2.99}          \n",
      "{'loss': 3.0816, 'learning_rate': 8.310249307479226e-07, 'epoch': 2.99}         \n",
      "{'loss': 2.8532, 'learning_rate': 5.54016620498615e-07, 'epoch': 2.99}          \n",
      "{'loss': 2.7333, 'learning_rate': 2.770083102493075e-07, 'epoch': 3.0}          \n",
      "{'loss': 2.3624, 'learning_rate': 0.0, 'epoch': 3.0}                            \n",
      "{'train_runtime': 1565.5247, 'train_samples_per_second': 0.525, 'train_steps_per_second': 0.525, 'train_loss': 2.764474816856013, 'epoch': 3.0}\n",
      "100%|| 822/822 [26:05<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "! python training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "def generate(prompt: str, max_new_tokens: int, temperature: float, do_sample: bool) -> None:\n",
    "        # Import the model\n",
    "#         config = PeftConfig.from_pretrained(hf_repo)\n",
    "        model = AutoModelForCausalLM.from_pretrained(config.model_name, return_dict=True, load_in_8bit=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        # Load the Lora model\n",
    "        model = PeftModel.from_pretrained(model, \"/scratch/sa6981/llm_unlearn/finetune_copyright/models/finetune_1.3b\")\n",
    "\n",
    "        # Generate text\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#         inputs.cuda()\n",
    "#         print(inputs)\n",
    "#         print(inputs.shape)\n",
    "        tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            top_p=0.95, top_k=50,\n",
    "        )\n",
    "        print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>The hobbits were so suprised seeing their friend again that they were all silent.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The hobbits were so suprised seeing their friend\"\n",
    "generate(prompt, 256, 0.3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>Get up, Sam! said Frodo. Youre not going to be left behind. Im not going to be left behind! said Sam. Ill go with you! No, said Frodo. You cant. Youre too weak. Im not weak, said Sam. Im not going to be left behind. Youre not going to be left behind, Sam! said Frodo. You\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Get up, Sam! said\"\n",
    "generate(prompt, 128, 0.3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate for whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset_dict.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 31\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_from_peft_1_3b(dataset,max_new_tokens: int, temperature: float, do_sample: bool):\n",
    "    model = AutoModelForCausalLM.from_pretrained(config.model_name, return_dict=True, load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    # Load the Lora model\n",
    "    model = PeftModel.from_pretrained(model, \"/scratch/sa6981/llm_unlearn/finetune_copyright/models/finetune_1.3b\")\n",
    "    tokenized_dataset = dataset.map(lambda example: example[\"input_ids\"], batched=True)\n",
    "    # Run the language model on the tokenized dataset\n",
    "    input_ids = tokenized_dataset.to(device)\n",
    "    print(input_ids)\n",
    "#     print(input_ids.shape)\n",
    "#     outputs = model.generate(input_ids)\n",
    "    tokens = model.generate(\n",
    "            input_ids = input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            top_p=0.95, top_k=50,\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seq = get_pred_from_peft_1_3b(dataset['test'],256, 0.3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming dataset['test'] is already tokenized and has 'input_ids'\n",
    "tokenized_dataset = dataset['test']\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "loader = DataLoader(tokenized_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# List to store generated outputs\n",
    "generated_outputs = []\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name, return_dict=True, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, \"/scratch/sa6981/llm_unlearn/finetune_copyright/models/finetune_1.3b\")\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in loader:\n",
    "    # Assuming 'input_ids' is the key for the input_ids feature\n",
    "    input_ids = batch['input_ids']# Move to the appropriate device (e.g., GPU)\n",
    "    input_ids_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
    "    print(len(input_ids))\n",
    "#     print(input_ids)\n",
    "    # Generate outputs from the language model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids_tensor,\n",
    "                                max_new_tokens=64,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.95, top_k=50,)\n",
    "\n",
    "    # Append the generated outputs to the list\n",
    "    generated_outputs.extend(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
