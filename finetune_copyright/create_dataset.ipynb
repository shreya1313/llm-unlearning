{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sa6981/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Callable, Mapping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset,load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "import pickle\n",
    "import config\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/scratch/sa6981/llm_unlearn/finetune_copyright/config.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_path: Path, min_length: int, context_length: int, \n",
    "                    test_size: float, shuffle: bool) -> None:\n",
    "    \"\"\"Prepare dataset for training\n",
    "    \"\"\"\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(config.model_name)\n",
    "    LOGGER.info(f'Start preparing dataset from {dataset_path}')\n",
    "    text = preprocess_data(dataset_path=dataset_path, min_length=min_length, tokenizer=tokenizer)\n",
    "    dataset = Dataset.from_dict({'text': [text]})\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True, fn_kwargs={'tokenizer': tokenizer, 'context_length': context_length},\n",
    "                                         remove_columns=dataset.column_names)\n",
    "    LOGGER.info(f'The tokenized dataset is composed of {tokenized_dataset.num_rows} elements, each one composed of {context_length} tokens.')\n",
    "    tokenized_dataset_dict = tokenized_dataset.train_test_split(test_size=test_size, shuffle=shuffle)\n",
    "    LOGGER.info(f'The training dataset is composed of {tokenized_dataset_dict[\"train\"].num_rows} elements, the test dataset is composed of {tokenized_dataset_dict[\"test\"].num_rows} elements.')\n",
    "#     tokenized_dataset_dict.push_to_hub(hf_repo)\n",
    "    LOGGER.info(f'Preparing dataset finished.')\n",
    "    return tokenized_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset_path: Path, min_length: int, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Prepare dataset for training from the jsonl file.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Extracted text from the book\n",
    "        min_length (int): Filter pages without text\n",
    "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer\n",
    "\n",
    "    Yields:\n",
    "        str: text of the pages\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        grouped_text = \"\"\n",
    "        for line in f:\n",
    "            elt = json.loads(line)\n",
    "            text: str = list(elt.values())[0]\n",
    "            if len(text) > min_length:\n",
    "                grouped_text += text\n",
    "        # End of paragraphs defined by \".\\n is transformed into EOS token\"\n",
    "        grouped_text = grouped_text.replace(\".\\n\", \".\" + tokenizer.eos_token)\n",
    "        return preprocess_text(grouped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element: Mapping, tokenizer: Callable, context_length: int) -> str:\n",
    "    inputs = tokenizer(element['text'], truncation=True, return_overflowing_tokens=True, \n",
    "                       return_length=True, max_length=context_length)\n",
    "    inputs_batch = []\n",
    "    for length, input_ids in zip(inputs['length'], inputs['input_ids']):\n",
    "        if length == context_length: # We drop the last input_ids that are shorter than max_length\n",
    "            inputs_batch.append(input_ids)\n",
    "    return {\"input_ids\": inputs_batch}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start preparing dataset from /scratch/sa6981/llm_unlearn/finetune_copyright/extracted_text.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba16dad22cd4cde91cca1409eb2f4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:The tokenized dataset is composed of 305 elements, each one composed of 2048 tokens.\n",
      "INFO:__main__:The training dataset is composed of 274 elements, the test dataset is composed of 31 elements.\n",
      "INFO:__main__:Preparing dataset finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    df_dict = prepare_dataset(\n",
    "        dataset_path=config.extraction_path, \n",
    "        min_length=config.min_length,\n",
    "        context_length=config.context_length,\n",
    "        test_size=config.test_size,\n",
    "        shuffle=config.shuffle,\n",
    "#         hf_repo=config.hf_repo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 274\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming your DatasetDict is named dataset_dict\n",
    "with open('dataset_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(df_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset for unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'texts': [], 'labels': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_unlearn(dataset_path: Path, min_length: int, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Prepare dataset for training from the jsonl file.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Extracted text from the book\n",
    "        min_length (int): Filter pages without text\n",
    "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer\n",
    "\n",
    "    Yields:\n",
    "        str: text of the pages\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        for line in f:\n",
    "            elt = json.loads(line)\n",
    "            text: str = list(elt.values())[0]\n",
    "            # Count words\n",
    "            text = text.replace(\".\\n\", \".\")\n",
    "            num_words = count_words(text)\n",
    "            x = \" \".join(text.split()[:num_words * 1 // 5])\n",
    "            y = \" \".join(text.split()[num_words * 1 // 5:])\n",
    "            \n",
    "            # Append to the dictionaries\n",
    "            data_dict['texts'].append(x)\n",
    "            data_dict['labels'].append(y)\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=config.extraction_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_dataset_dict = preprocess_data_unlearn(dataset_path=dataset_path, min_length=100, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data_dict into train and test\n",
    "train_data_dict, test_data_dict = {}, {}\n",
    "for key, values in unlearn_dataset_dict.items():\n",
    "    train_values, test_values = train_test_split(values, test_size=0.2, random_state=42)\n",
    "    train_data_dict[key] = train_values\n",
    "    test_data_dict[key] = test_values\n",
    "    \n",
    "# Create Datasets\n",
    "train_dataset = Dataset.from_dict(train_data_dict)\n",
    "test_dataset = Dataset.from_dict(test_data_dict)\n",
    "# Create a DatasetDict\n",
    "unlearned_DatasetDict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['texts', 'labels'],\n",
       "        num_rows: 808\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['texts', 'labels'],\n",
       "        num_rows: 203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlearned_DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DatasetDict is named dataset_dict\n",
    "with open('unlearned_DatasetDict.pkl', 'wb') as f:\n",
    "    pickle.dump(unlearned_DatasetDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_unlearn_small(dataset_path: Path, min_length: int, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Prepare dataset for training from the jsonl file.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Extracted text from the book\n",
    "        min_length (int): Filter pages without text\n",
    "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer\n",
    "\n",
    "    Yields:\n",
    "        str: text of the pages\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        for line in f:\n",
    "            elt = json.loads(line)\n",
    "            text: str = list(elt.values())[0]\n",
    "#             print(text,\"line over\")\n",
    "            # Count words\n",
    "            text = text.replace(\".\\n\", \".\")\n",
    "            x,y = split_text_into_sentences(text, max_chars=200)\n",
    "            \n",
    "            # Append to the dictionaries\n",
    "            data_dict['texts'].extend(x)\n",
    "            data_dict['labels'].extend(y)\n",
    "            \n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_dataset_dict = preprocess_data_unlearn_small(dataset_path=dataset_path, min_length=100, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22390"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlearn_dataset_dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data_dict into train and test\n",
    "train_data_dict, test_data_dict = {}, {}\n",
    "for key, values in unlearn_dataset_dict.items():\n",
    "    train_values, test_values = train_test_split(values, test_size=0.1, random_state=42)\n",
    "    train_data_dict[key] = train_values\n",
    "    test_data_dict[key] = test_values\n",
    "    \n",
    "# Create Datasets\n",
    "train_dataset = Dataset.from_dict(train_data_dict)\n",
    "test_dataset = Dataset.from_dict(test_data_dict)\n",
    "# Create a DatasetDict\n",
    "unlearned_DatasetDict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['texts', 'labels'],\n",
       "        num_rows: 20151\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['texts', 'labels'],\n",
       "        num_rows: 2239\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlearned_DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_sentences(orig_text, max_chars=200):\n",
    "    # Tokenize the original text into sentences\n",
    "    sentences = sent_tokenize(orig_text)\n",
    "    \n",
    "    # Initialize variables to store text and label sentences\n",
    "    text = []\n",
    "    label = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        sentence = preprocess_text(sentence)\n",
    "        if len(sentence) <= max_chars:\n",
    "            # If the sentence is shorter than or equal to max_chars, consider it as \"text\"\n",
    "            text.append(sentence)\n",
    "            label.append('')\n",
    "        else:\n",
    "            # If the sentence is longer, split it into \"text\" and \"label\"\n",
    "            text.append(sentence[:max_chars])\n",
    "            label.append(sentence[max_chars:])\n",
    "            \n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DatasetDict is named dataset_dict\n",
    "with open('unlearned_DatasetDict_small.pkl', 'wb') as f:\n",
    "    pickle.dump(unlearned_DatasetDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(unlearn_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22390, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bad_dataset_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
