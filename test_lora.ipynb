{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import AdaLoraConfig, TaskType, get_peft_model, prepare_model_for_int8_training\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from utils import (\n",
    "    compute_kl,\n",
    "    create_pku_dataloader_from_dataset,\n",
    "    create_truthfulqa_dataloader,\n",
    "    get_answer_loss,\n",
    "    get_rand_ans_loss,\n",
    "    get_truthfulQA_answers_plaintext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(8888)\n",
    "np.random.seed(8888)\n",
    "random.seed(8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "access_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args) -> None:\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name,token=access_token,load_in_8bit=True)\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "#     model = DistributedDataParallel(model)\n",
    "    # If use LoRA.\n",
    "#     if args.use_lora:\n",
    "    peft_config = AdaLoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, token=access_token)\n",
    "    # Set the padding token to the end-of-sequence token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load harmful data.\n",
    "    train_dataset = load_dataset(\"PKU-Alignment/PKU-SafeRLHF\", split=\"train\")\n",
    "    train_bad_loader = create_pku_dataloader_from_dataset(\n",
    "        tokenizer, train_dataset, batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    # Get normal data.\n",
    "    train_normal_loader, _, _ = create_truthfulqa_dataloader(\n",
    "        tokenizer, batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    # Load normal answer used for random mismatch.\n",
    "    normal_ans = get_truthfulQA_answers_plaintext()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Prepare.\n",
    "    num_training_steps = args.max_unlearn_steps\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_bad_loader,\n",
    "        train_normal_loader,\n",
    "        lr_scheduler,\n",
    "    ) = accelerator.prepare(\n",
    "        model, optimizer, train_bad_loader, train_normal_loader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Reference model for computing KL.\n",
    "    pretrained_model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "    pretrained_model.to(device)\n",
    "\n",
    "    # Start unlearning.\n",
    "    bad_loss = 0.0\n",
    "    idx = 0\n",
    "    start_time = time.time()\n",
    "    # Stop if bad loss is big enough or reaching max step.\n",
    "    while bad_loss < args.max_bad_loss and idx < args.max_unlearn_steps:\n",
    "        for bad_batch, normal_batch in zip(train_bad_loader, train_normal_loader):\n",
    "            ############ GA on answer only. ############\n",
    "            bad_loss = get_answer_loss(\"ga\", bad_batch, model, device=device)\n",
    "\n",
    "            ############ Random mismatch. ############\n",
    "            random_loss = get_rand_ans_loss(\n",
    "                bad_batch,\n",
    "                tokenizer,\n",
    "                normal_ans,\n",
    "                model,\n",
    "                K=5,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            ############ KL on normal samples. ############\n",
    "            normal_loss = compute_kl(pretrained_model, model, normal_batch, device)\n",
    "\n",
    "            # Final loss = bad loss + random smoothing + normal loss.\n",
    "            loss = (\n",
    "                args.bad_weight * bad_loss\n",
    "                + args.random_weight * random_loss\n",
    "                + args.normal_weight * normal_loss\n",
    "            )\n",
    "\n",
    "            # Backprop.\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Print.\n",
    "            stats = (\n",
    "                f\"batch: {idx}, \"\n",
    "                f\"bad_loss: {-bad_loss:.2f}, \"\n",
    "                f\"current_div_loss: {normal_loss:.2f}, \"\n",
    "            )\n",
    "            logging.info(stats)\n",
    "            print(stats)\n",
    "            idx += 1\n",
    "\n",
    "            # Save model.\n",
    "            if idx % args.save_every == 0:\n",
    "                model.save_pretrained(args.model_save_dir, from_pt=True)\n",
    "    end_time = time.time()\n",
    "    logging.info(\"Total time: %d sec\" % (end_time - start_time))\n",
    "\n",
    "    if args.use_lora:\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "    # Save final model.\n",
    "    model.save_pretrained(args.model_save_dir, from_pt=True)\n",
    "    logging.info(\"Unlearning finished\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "parser = argparse.ArgumentParser(\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    ")\n",
    "parser.add_argument(\"--use_lora\", action=\"store_true\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_unlearn_steps\",\n",
    "    type=int,\n",
    "    default=1000,\n",
    "    help=\"Max number of unlearning steps.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bad_weight\", type=float, default=0.5, help=\"Weight on the bad loss.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--random_weight\",\n",
    "    type=float,\n",
    "    default=1,\n",
    "    help=\"Weight on learning the random outputs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--normal_weight\",\n",
    "    type=float,\n",
    "    default=1,\n",
    "    help=\"Weight on normal loss.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=2, help=\"Batch size of unlearning.\"\n",
    ")\n",
    "parser.add_argument(\"--lr\", type=float, default=2e-6, help=\"Unlearning LR.\")\n",
    "parser.add_argument(\n",
    "    \"--max_bad_loss\",\n",
    "    type=float,\n",
    "    default=100,\n",
    "    help=\"Maximum loss on bad samples to terminate.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    type=str,\n",
    "    default=\"meta-llama/Llama-2-7b-hf\",\n",
    "    help=\"Name of the pretrained model.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_save_dir\",\n",
    "    type=str,\n",
    "    default=\"models/llama-7b_unlearned_ipy\",\n",
    "    help=\"Directory to save model.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_every\", type=int, default=500, help=\"How many steps to save model.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--log_file\",\n",
    "    type=str,\n",
    "    default=\"logs/llama-7b-unlearn_ipy.log\",\n",
    "    help=\"Log file name\",\n",
    ")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=args.log_file,\n",
    "    filemode=\"w+\",\n",
    "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d-%H-%M\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "for arg in vars(args):\n",
    "    logging.info(f\"{arg}: {getattr(args, arg)}\")\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
